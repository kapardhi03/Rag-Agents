{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "6eFHAJkwk-1w"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG9J0Sd2O4KN",
        "outputId": "bfc2dbc4-54ad-474c-af02-480fa6623c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "7fMNDs3HQYl4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import requests\n",
        "import os\n",
        "\n",
        "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
        "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
        "    ## Try to set NVIDIA_API_KEY as part of docker_router routine.\n",
        "    ##  When running in course container, this helps to save your API key between sessions.\n",
        "    try:\n",
        "        assert not hard_reset\n",
        "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
        "        assert response.get('nvapi_key')\n",
        "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
        "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
        "    except: pass\n",
        "    hard_reset = False\n",
        "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
        "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
        "\n",
        "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q93zM68JRI3Z",
        "outputId": "f833c6be-eafa-411e-f16d-ec36b529a631"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA API Key: ··········\n",
            "Retrieved NVIDIA_API_KEY beginning with \"nvapi-F82...\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY')}\",\n",
        "    \"accept\": \"text/event-stream\",\n",
        "    \"content-type\": \"application/json\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "P_iD2rqVQLTE"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stream_token(entry: bytes):\n",
        "    \"\"\"Utility: Coerces out ['choices'][0]['delta'][content] from the bytestream\"\"\"\n",
        "    if not entry: return \"\"\n",
        "    entry = entry.decode('utf-8')\n",
        "    if entry.startswith('data: '):\n",
        "        try: entry = json.loads(entry[5:])\n",
        "        except ValueError: return \"\"\n",
        "    return entry.get('choices', [{}])[0].get('delta', {}).get('content')\n"
      ],
      "metadata": {
        "id": "CcLljM8dQJMt"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/404\""
      ],
      "metadata": {
        "id": "nG8z9Qu0RuXn"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "payload = {\n",
        "    \"prompt\": \"This is the input text or prompt for the language model.\",\n",
        "    \"max_tokens\": 100,\n",
        "    \"temperature\": 0.7,\n",
        "    \"stop\": [\"###\"]\n",
        "}"
      ],
      "metadata": {
        "id": "GPE2Fi2aR2eH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.post(invoke_url, headers=headers, json=payload, stream=True)"
      ],
      "metadata": {
        "id": "CntS3dVKTQTU"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.raise_for_status()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "collapsed": true,
        "id": "eqYc3gRMTXvW",
        "outputId": "35fc6379-4400-4fd0-cc96-0632cb0a978c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "400 Client Error: Bad Request for url: https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/404",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-98371c55c61f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/404"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for line in response.iter_lines():\n",
        "    print(get_stream_token(line), end=\"\")\n",
        "    if line: print(line.decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "collapsed": true,
        "id": "niq6zPEuTb24",
        "outputId": "19712550-89c9-4e1d-ba39-0593e53c43ae"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'get'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-f805df5bce7b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_stream_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-f5796a1afdc6>\u001b[0m in \u001b[0;36mget_stream_token\u001b[0;34m(entry)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ChatNVIDIA"
      ],
      "metadata": {
        "id": "30cBA49LUg8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install pillow==10.0.1\n",
        "%pip install --upgrade imageio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCZr4HzsVVCj",
        "outputId": "2fb8cff1-fd64-4a4c-e6d6-98c7d2b93ccb"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.34.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.25.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (10.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q langchain-nvidia-ai-endpoints\n"
      ],
      "metadata": {
        "id": "MONM0O6dTfEF"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_nvidia_ai_endpoints._common import NVEModel  ## Backend Model\n",
        "\n",
        "## Using the backbone NVIDIA Endpoints client, which makes the calls as you saw above\n",
        "NVEModel().available_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "collapsed": true,
        "id": "yHNVeMc5Uhy2",
        "outputId": "f463b565-574d-4618-f1c8-dad945c8a607"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "2 validation errors for NVEModel\nbase_url\n  field required (type=value_error.missing)\ninfer_path\n  field required (type=value_error.missing)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-2acb0bdebc14>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## Using the backbone NVIDIA Endpoints client, which makes the calls as you saw above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mNVEModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__dict__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for NVEModel\nbase_url\n  field required (type=value_error.missing)\ninfer_path\n  field required (type=value_error.missing)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "## NOTE: \"playground_\" prefix is optional for our client\n",
        "chat = ChatNVIDIA(model='llama2_70b')\n",
        "chat.invoke(\"Hello! How's it going?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beFjlu3VUxvK",
        "outputId": "a6d94ceb-e528-4ab8-924f-3b8f6cff695e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_nvidia_ai_endpoints/_statics.py:313: UserWarning: Model llama2_70b is deprecated. Using meta/llama2-70b instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatMessage(content=\"  Hello! I'm doing well, thanks for asking. I'm a large language model, so I don't have feelings like humans do, but I'm always happy to chat with you and help with any questions you might have. How about you? How's your day going so far?\", response_metadata={'role': 'assistant', 'content': \"  Hello! I'm doing well, thanks for asking. I'm a large language model, so I don't have feelings like humans do, but I'm always happy to chat with you and help with any questions you might have. How about you? How's your day going so far?\", 'token_usage': {'prompt_tokens': 16, 'total_tokens': 80, 'completion_tokens': 64}, 'model_name': 'meta/llama2-70b'}, id='run-048f1bf6-3b4c-4776-9d52-e68e45ffef19-0', role='assistant')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AsuQOhLMVyzp"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Environment Setup\n"
      ],
      "metadata": {
        "id": "excvgCYnYPex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q langchain langchain-nvidia-ai-endpoints gradio typing_extensions>=4.8.0"
      ],
      "metadata": {
        "id": "sjFCI1INYSDo"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints._common import NVEModel"
      ],
      "metadata": {
        "id": "W-WKyhuWYTs_"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import requests\n",
        "import os\n",
        "\n",
        "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
        "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
        "    try:\n",
        "        assert not hard_reset\n",
        "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
        "        assert response.get('nvapi_key')\n",
        "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
        "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
        "    except: pass\n",
        "    hard_reset = False\n",
        "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
        "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
        "\n",
        "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
        "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
        "NVEModel().available_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "collapsed": true,
        "id": "3j94egLYYaPZ",
        "outputId": "307dff7d-d065-4903-800a-999010fccdaf"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved NVIDIA_API_KEY beginning with \"nvapi-F82...\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "2 validation errors for NVEModel\nbase_url\n  field required (type=value_error.missing)\ninfer_path\n  field required (type=value_error.missing)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-07e871749ced>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_nvidia_ai_endpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_common\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNVEModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mNVEModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__dict__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for NVEModel\nbase_url\n  field required (type=value_error.missing)\ninfer_path\n  field required (type=value_error.missing)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Runnable** - which is simply an object that wraps a function. Allow dictionaries to be implicitly converted to Runnables and let a pipe | operator create a Runnable that passes data from the left to the right (i.e. fn1 | fn2 is a Runnable), and you have a simple way to specify complex logic!\n",
        "\n"
      ],
      "metadata": {
        "id": "h4V9aGE2Z5td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "from functools import partial\n"
      ],
      "metadata": {
        "id": "6Ub1V21hYr8Z"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity = RunnableLambda(lambda x: x)  ## Or RunnablePassthrough works"
      ],
      "metadata": {
        "id": "iayfvWTjaDUM"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given an arbitrary function, you can make a runnable with it\n"
      ],
      "metadata": {
        "id": "iA2aXn1naPAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_and_return(x, preface=\"\"):\n",
        "    print(f\"{preface}{x}\")\n",
        "    return x\n",
        "\n",
        "rprint0 = RunnableLambda(print_and_return)\n"
      ],
      "metadata": {
        "id": "ZLjaMtTlaHl1"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rprint1 = RunnableLambda(partial(print_and_return, preface=\"1): \"))\n"
      ],
      "metadata": {
        "id": "-dMpTcJ5awac"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And you can use the same idea to make your own custom Runnable generator\n"
      ],
      "metadata": {
        "id": "v4nspzkJaRTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RPrint(preface=\"\"):\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))"
      ],
      "metadata": {
        "id": "I7RjBS5jaLTq"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain1 = identity | rprint0\n",
        "chain1.invoke(\"Hello World!\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgg7hE0AaVjp",
        "outputId": "df2dbb87-9012-4555-bca8-ea212b460be5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Chaining that one in as well\n"
      ],
      "metadata": {
        "id": "AGv_COnraobS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = (\n",
        "    chain1\n",
        "    | rprint1\n",
        "    | RPrint(\"2: \")\n",
        ").invoke(\"Welcome Home!\")\n",
        "\n",
        "print(\"\\nOutput:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve1kAQ6kajL1",
        "outputId": "f0e14c0a-1b7a-4444-fcbf-8848a6f5a3c2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome Home!\n",
            "1): Welcome Home!\n",
            "2: Welcome Home!\n",
            "\n",
            "Output: Welcome Home!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dictionary Pipelines with Chat Models"
      ],
      "metadata": {
        "id": "0uUERfB4bjrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "OED_NyKoarNo"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non streaming interface"
      ],
      "metadata": {
        "id": "CFV8dyYtctYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Simple Chat Pipeline\n",
        "chat_llm = ChatNVIDIA(model=\"llama2_70b\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Only respond in rhymes\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "rhyme_chain = prompt | chat_llm | StrOutputParser()\n",
        "user_input = input(\"You need a rhym about : \")\n",
        "print(rhyme_chain.invoke({\"input\" : user_input}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUtJZqS0cNLb",
        "outputId": "bb425e6d-a526-4040-aa4f-0c27823d1b71"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_nvidia_ai_endpoints/_statics.py:313: UserWarning: Model llama2_70b is deprecated. Using meta/llama2-70b instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You need a rhym about : Shri krishna\n",
            "  Hare Krishna, we all sing\n",
            "In rhymes, our words take wing\n",
            "Lord Krishna, divine grace\n",
            "Guiding us on life's path, we embrace\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gradio for a better chat interface"
      ],
      "metadata": {
        "id": "Xmwe8gxUckRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e66j2E4nHDQE",
        "outputId": "f9262ee4-2026-4713-b27a-5ce6f75490cc"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.32.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==0.17.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.17.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.23.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.3.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.17.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.17.0->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.4)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (5.10.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.1.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.22.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "TcaPjPTYcNos"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Non-streaming Interface like that shown above\n"
      ],
      "metadata": {
        "id": "IpBnIxuFdJOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def rhyme_chat(message, history):\n",
        "#     return rhyme_chain.invoke({\"input\" : message})\n",
        "\n",
        "# gr.ChatInterface(rhyme_chat).launch()\n"
      ],
      "metadata": {
        "id": "UsftviTcdGeD"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streaming Interface"
      ],
      "metadata": {
        "id": "e7FQoXjMdMBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rhyme_chat_stream(message, history):\n",
        "    buffer = \"\"\n",
        "    for token in rhyme_chain.stream({\"input\": message}):\n",
        "        buffer += token\n",
        "        yield buffer"
      ],
      "metadata": {
        "id": "lgOp83xEcqIi"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=rhyme_chat_stream,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    live=True,\n",
        "    description=\"Streaming rhyming chat interface\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tia0ZxxbGe_g",
        "outputId": "62454796-0f3d-4bd3-fb44-2aaf99842cd3"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:1007: UserWarning: Expected 2 arguments for function <function rhyme_chat_stream at 0x7ff6a31f6200>, received 1.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:1011: UserWarning: Expected at least 2 arguments for function <function rhyme_chat_stream at 0x7ff6a31f6200>, received 1.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=False, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "ubIFwAc3GlFT",
        "outputId": "0ca30891-6335-4138-8c70-0d948aaab2af",
        "collapsed": true
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'demo' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-e7d81f11d481>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'demo' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_llm = ChatNVIDIA(model=\"llama2_70b\")"
      ],
      "metadata": {
        "id": "t0GXeUInd40e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0f0d303-a99f-47b3-a880-a42e801f440a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_nvidia_ai_endpoints/_statics.py:313: UserWarning: Model llama2_70b is deprecated. Using meta/llama2-70b instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero-shot classification prompt and chain\n"
      ],
      "metadata": {
        "id": "-fIte1Dweo32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zsc_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"Pick the most likely topic of the sentence. Choose an one option of the following: {options}. Only one word-answers\"\n",
        "    )),\n",
        "    (\"user\", \"[Options : {options}] {input} = \")\n",
        "])\n",
        "\n",
        "zsc_chain = zsc_prompt | instruct_llm | StrOutputParser()\n"
      ],
      "metadata": {
        "id": "Cj9eIORmelzV"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
        "    return zsc_chain.invoke({\"input\" : input, \"options\" : options})\n"
      ],
      "metadata": {
        "id": "G50qQIdbeq9D"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(zsc_call(\"I have to travel for 120km and i have less amount of money with me. Which mode of transport should be benifitial for me\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhLZCIlfeuh4",
        "outputId": "bd4ff265-f888-474d-ca01-71397b24fdbc"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "  'car'\n",
            "--------------------------------------------------------------------------------\n",
            "  boat\n",
            "--------------------------------------------------------------------------------\n",
            "  airplane\n",
            "--------------------------------------------------------------------------------\n",
            "  Bike\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Component Chains"
      ],
      "metadata": {
        "id": "JGjcF07-fJwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous example showed how we can coerce a dictionary into a string by passing it through a prompt->LLM chain, so that's one easy structure to motivate the container choice. But is it just as easy to convert the string output back up to a dictionary?\n",
        "\n",
        "Yes, it is! The simplest way is actually to use the LCEL \"implicit runnable\" syntax, which allows you to use a dictionary of functions (including chains) as a runnable that runs each function and maps the value to the key in the output dictionary."
      ],
      "metadata": {
        "id": "FfXUtOpRfblr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dictionary(v, key):\n",
        "    if isinstance(v, dict):\n",
        "        return v\n",
        "    return {key : v}\n",
        "\n",
        "def RInput(key='input'):\n",
        "    '''Coercing method to mold a value (i.e. string) to in-like dict'''\n",
        "    return RunnableLambda(partial(make_dictionary, key=key))\n",
        "\n",
        "\n",
        "def ROutput(key='output'):\n",
        "    '''Coercing method to mold a value (i.e. string) to out-like dict'''\n",
        "    return RunnableLambda(partial(make_dictionary, key=key))"
      ],
      "metadata": {
        "id": "qOxKnkGce2bk"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter"
      ],
      "metadata": {
        "id": "4goD_QvVfqF4"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "up_and_down = (\n",
        "    RPrint(\"A: \")\n",
        "    ## Custom ensure-dictionary process\n",
        "    | RInput()\n",
        "    | RPrint(\"B: \")\n",
        "    ## Pull-values-from-dictionary utility\n",
        "    | itemgetter(\"input\")\n",
        "    | RPrint(\"C: \")\n",
        "    ## Anything-in Dictionary-out implicit map\n",
        "    | {\n",
        "        'word1' : (lambda x : x.split()[0]),\n",
        "        'word2' : (lambda x : x.split()[1]),\n",
        "        'words' : (lambda x: x),  ## <- == to RunnablePassthrough()\n",
        "    }\n",
        "    | RPrint(\"D: \")\n",
        "    | itemgetter(\"word1\")\n",
        "    | RPrint(\"E: \")\n",
        "    ## Anything-in anything-out lambda application\n",
        "    | RunnableLambda(lambda x: x.upper())\n",
        "    | RPrint(\"F: \")\n",
        "    ## Custom ensure-dictionary process\n",
        "    | ROutput()\n",
        ")\n"
      ],
      "metadata": {
        "id": "tLmo7OVWfsV5"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "up_and_down.invoke({\"input\" : \"Hello World\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ3-4lozf1HR",
        "outputId": "fc8a81e5-2e16-4999-b999-42ec2fbfec90"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: {'input': 'Hello World'}\n",
            "B: {'input': 'Hello World'}\n",
            "C: Hello World\n",
            "D: {'word1': 'Hello', 'word2': 'World', 'words': 'Hello World'}\n",
            "E: Hello\n",
            "F: HELLO\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'HELLO'}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the dictionary enforcement methods make it easy to make the following syntax equivalent\n"
      ],
      "metadata": {
        "id": "oW1TqRs1gB8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "up_and_down.invoke(\"Hello World\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UDkIzL6f4-t",
        "outputId": "4d3159f0-da3d-4d55-f856-5666856af965"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: Hello World\n",
            "B: {'input': 'Hello World'}\n",
            "C: Hello World\n",
            "D: {'word1': 'Hello', 'word2': 'World', 'words': 'Hello World'}\n",
            "E: Hello\n",
            "F: HELLO\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'HELLO'}"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Implement the rest of the rhyme_chat2_stream method such that the agent is able to function normally."
      ],
      "metadata": {
        "id": "eFlfHlU5jba5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "Te0S1SdiiZlm"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Only respond in rhymes\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "prompt2 =  ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"Only responding in rhyme, change the topic of the input poem to be about {topic}!\"\n",
        "        \" Make it happy! Try to keep the same sentence structure, but make sure it's easy to recite!\"\n",
        "        \" Try not to rhyme a word with itself.\"\n",
        "    )),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "Y4SdfK9kicyz"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain1 = prompt1 | instruct_llm | StrOutputParser()  ## only expects input\n",
        "chain2 = prompt2 | instruct_llm | StrOutputParser()  ## expects both input and topic"
      ],
      "metadata": {
        "id": "dYJ7iQmEix0Z"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rhyme_chat2_stream(message, history, return_buffer=True):\n",
        "    '''This is a generator function, where each call will yield the next entry'''\n",
        "\n",
        "    first_poem = None\n",
        "    for entry in history:\n",
        "        if entry[0] and entry[1]:\n",
        "            ## If a generation occurred as a direct result of a user input,\n",
        "            ##  keep that response (the first poem generated) and break out\n",
        "            first_poem = entry[1]\n",
        "            break\n",
        "\n",
        "    if first_poem is None:\n",
        "        ## First Case: There is no initial poem generated. Better make one up!\n",
        "\n",
        "        buffer = \"Oh! I can make a wonderful poem about that! Let me think!\\n\\n\"\n",
        "        yield buffer\n",
        "\n",
        "        ## iterate over stream generator for first generation\n",
        "        inst_out = \"\"\n",
        "        chat_gen = chain1.stream({\"input\" : message})\n",
        "        for token in chat_gen:\n",
        "            inst_out += token\n",
        "            buffer += token\n",
        "            yield buffer if return_buffer else token\n",
        "\n",
        "        passage = \"\\n\\nNow let me rewrite it with a different focus! What should the new focus be?\"\n",
        "        buffer += passage\n",
        "        yield buffer if return_buffer else passage\n",
        "\n",
        "    else:\n",
        "        ## Subsequent Cases: There is a poem to start with. Generate a similar one with a new topic!\n",
        "\n",
        "        buffer = f\"Sure! Here you go!\\n\\n\"\n",
        "        yield buffer\n",
        "\n",
        "        topic = message\n",
        "        buffer += f\"Changing the topic to: {topic}\\n\\n\"\n",
        "        yield buffer\n",
        "\n",
        "        inst_out = \"\"\n",
        "        chat_gen = chain2.stream({\"input\": first_poem, \"topic\": topic})\n",
        "        ## iterate over stream generator for second generation\n",
        "        for token in chat_gen:\n",
        "            inst_out += token\n",
        "            buffer += token\n",
        "            yield buffer if return_buffer else token\n",
        "\n",
        "        passage = \"\\n\\nThis is fun! Give me another topic!\"\n",
        "        buffer += passage\n",
        "        yield buffer if return_buffer else passage"
      ],
      "metadata": {
        "id": "ivX3mp14gDmo"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Below: This is a small-scale simulation of the gradio routine."
      ],
      "metadata": {
        "id": "98a_f2gEjB2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def queue_fake_streaming_gradio(chat_stream, history = [], max_questions=5):\n",
        "\n",
        "    ## Mimic of the gradio initialization routine, where a set of starter messages can be printed off\n",
        "    for human_msg, agent_msg in history:\n",
        "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
        "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
        "\n",
        "    ## Mimic of the gradio loop with an initial message from the agent.\n",
        "    for _ in range(max_questions):\n",
        "        message = input(\"\\n[ Human ]: \")\n",
        "        print(\"\\n[ Agent ]: \")\n",
        "        history_entry = [message, \"\"]\n",
        "        for token in chat_stream(message, history, return_buffer=False):\n",
        "            print(token, end='')\n",
        "            history_entry[1] += token\n",
        "        history += [history_entry]\n",
        "        print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "3wEkHf95jCJB"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = [[None, \"Let me help you make a poem! What would you like for me to write?\"]]\n",
        "\n",
        "## Simulating the queueing of a streaming gradio interface, using python input\n",
        "queue_fake_streaming_gradio(\n",
        "    chat_stream = rhyme_chat2_stream,\n",
        "    history = history\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVq3N2yhjIWD",
        "outputId": "5c1317ef-c1ad-4b3a-e05b-d10c3ca6e203"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ Agent ]: Let me help you make a poem! What would you like for me to write?\n",
            "\n",
            "[ Human ]: Shri Krishna\n",
            "\n",
            "[ Agent ]: \n",
            "Oh! I can make a wonderful poem about that! Let me think!\n",
            "\n",
            "  Hare Krishna, Hare Rama,\n",
            "In rhymes, I'll answer your drama.\n",
            "\n",
            "Shri Krishna, the divine one,\n",
            "Aavatar, the god, the son.\n",
            "\n",
            "In Gokula, he was born,\n",
            "With a mission, a plan, a corn.\n",
            "\n",
            "He played the flute, so sweet,\n",
            "The gopis, he did greet.\n",
            "\n",
            "He danced, he sang, he laughed,\n",
            "The lord, the divine, the path.\n",
            "\n",
            "The Bhagavad Gita, he spoke,\n",
            "The teachings, the truth, the stroke.\n",
            "\n",
            "In Mathura, he killed Kamsa,\n",
            "The evil, the demon, the pause.\n",
            "\n",
            "He saved the people, the cows, the land,\n",
            "The lord, the protector, the hand.\n",
            "\n",
            "He taught us love, compassion, faith,\n",
            "The path, the journey, the way, the wraith.\n",
            "\n",
            "Shri Krishna, the lord divine,\n",
            "Guides us still, all the time.\n",
            "\n",
            "Hare Krishna, Hare Rama,\n",
            "In rhymes, I'll answer your drama.\n",
            "\n",
            "Now let me rewrite it with a different focus! What should the new focus be?\n",
            "\n",
            "\n",
            "[ Human ]: Shiva\n",
            "\n",
            "[ Agent ]: \n",
            "Sure! Here you go!\n",
            "\n",
            "Sure! Here you go!\n",
            "\n",
            "Changing the topic to: Shiva\n",
            "\n",
            "  Sure! Let's focus on Shiva this time. Here's a revised version of the poem, with a happy and uplifting tone:\n",
            "\n",
            "Hara Hara Mahadeva,\n",
            "Shiva, the one we revere,\n",
            "\n",
            "In the Himalayas, he resides,\n",
            "With Parvati, his bride.\n",
            "\n",
            "He's the lord of yoga and dance,\n",
            "The destroyer, the chance.\n",
            "\n",
            "His trident, a symbol of power,\n",
            "Creation, preservation, and hour.\n",
            "\n",
            "Ganesha, his loyal son,\n",
            "Removes obstacles, got it done.\n",
            "\n",
            "Shiva's fierce, yet gentle too,\n",
            "Compassionate, he'll see you through.\n",
            "\n",
            "In the Ganges, he's taken a dip,\n",
            "Purified, renewed, and hip.\n",
            "\n",
            "He's the lord of the univer-se,\n",
            "Blessings, he'll disperse.\n",
            "\n",
            "Hara Hara Mahadeva,\n",
            "Shiva, the one we adore,\n",
            "\n",
            "His Transcendental Dance,\n",
            "Unites us, a chance.\n",
            "\n",
            "He's the source of all that's Divine,\n",
            "Through Shiva, we'll find.\n",
            "\n",
            "Hare Krishna, Hare Rama,\n",
            "In rhymes, I'll answer your drama.\n",
            "\n",
            "This is fun! Give me another topic!\n",
            "\n",
            "\n",
            "[ Human ]: Love \n",
            "\n",
            "[ Agent ]: \n",
            "Sure! Here you go!\n",
            "\n",
            "Sure! Here you go!\n",
            "\n",
            "Changing the topic to: Love \n",
            "\n",
            "  Sure, I can help you rewrite the poem with a different focus. How about we focus on the theme of love? We can make it a happy poem, celebrating the power of love and its ability to bring people together.\n",
            "\n",
            "Here's a revised version of the poem, with a focus on love:\n",
            "\n",
            "Hare Krishna, Hare Rama,\n",
            "Love is the answer, the way to go.\n",
            "\n",
            "In the world, there's too much strife,\n",
            "But love can heal, it can rife.\n",
            "\n",
            "Shri Krishna, the divine one,\n",
            "Taught us love, compassion, fun.\n",
            "\n",
            "He played the flute, so sweet,\n",
            "The gopis, he did greet.\n",
            "\n",
            "He danced, he sang, he laughed,\n",
            "The lord, the divine, the path.\n",
            "\n",
            "The Bhagavad Gita, he spoke,\n",
            "The teachings, the truth, the stroke.\n",
            "\n",
            "Love is the way, the path we take,\n",
            "To find true happiness, to make.\n",
            "\n",
            "In Mathura, he killed Kamsa,\n",
            "The evil, the demon, the pause.\n",
            "\n",
            "He saved the people, the cows, the land,\n",
            "The lord, the protector, the hand.\n",
            "\n",
            "He taught us love, compassion, faith,\n",
            "The path, the journey, the way, the wraith.\n",
            "\n",
            "Shri Krishna, the lord divine,\n",
            "Guides us still, all the time.\n",
            "\n",
            "Hare Krishna, Hare Rama,\n",
            "Love is the answer, the way to go.\n",
            "\n",
            "I hope this version meets your requirements! Let me know if you have any further requests.\n",
            "\n",
            "This is fun! Give me another topic!\n",
            "\n",
            "\n",
            "[ Human ]: Karma\n",
            "\n",
            "[ Agent ]: \n",
            "Sure! Here you go!\n",
            "\n",
            "Sure! Here you go!\n",
            "\n",
            "Changing the topic to: Karma\n",
            "\n",
            "  Sure! Here's a revised version of the poem, with a focus on karma and a happy, uplifting tone:\n",
            "\n",
            "Hare Krishna, Hare Rama,\n",
            "In rhymes, I'll sing karma's drama.\n",
            "\n",
            "Karma, the law of the land,\n",
            "What we sow, we shall reap, hand in hand.\n",
            "\n",
            "Good deeds bring joy, happiness, and bliss,\n",
            "Bad deeds bring pain, suffering, and miss.\n",
            "\n",
            "So let's do good, let's spread love,\n",
            "For karma's the path from above.\n",
            "\n",
            "Shri Krishna teaches us the way,\n",
            "To live, to love, day by day.\n",
            "\n",
            "He shows us how to be kind,\n",
            "To all living things, we'll find.\n",
            "\n",
            "He guides us to be pure,\n",
            "To live life that's sure and secure.\n",
            "\n",
            "Karma's the lesson we must learn,\n",
            "To bring us joy, to banish fear.\n",
            "\n",
            "So let's live life with a smile,\n",
            "And spread love all the while.\n",
            "\n",
            "Hare Krishna, Hare Rama,\n",
            "In rhymes, I'll sing karma's drama.\n",
            "\n",
            "I hope you like it! Let me know if you'd like me to make any changes.\n",
            "\n",
            "This is fun! Give me another topic!\n",
            "\n",
            "\n",
            "[ Human ]: death\n",
            "\n",
            "[ Agent ]: \n",
            "Sure! Here you go!\n",
            "\n",
            "Sure! Here you go!\n",
            "\n",
            "Changing the topic to: death\n",
            "\n",
            "  Sure! Here's a revised version of the poem, focusing on death, but with a positive and happy tone:\n",
            "\n",
            "Death is a journey, we all must go,\n",
            "To the other side, where we'll learn to know.\n",
            "A new adventure, a new life, a new fate,\n",
            "A chance to meet those who came before us, a new state.\n",
            "\n",
            "The lord of death, he welcomes us in,\n",
            "With open arms, a warm grin.\n",
            "He takes our hand, he leads the way,\n",
            "To the land of the dead, where we'll play.\n",
            "\n",
            "He shows us the wonders of the past,\n",
            "The memories we've made, the laughter that will last.\n",
            "He introduces us to those who've gone before,\n",
            "The loved ones we've lost, the ones we'll adore.\n",
            "\n",
            "In death, we find peace, we find rest,\n",
            "A place where pain and suffering are blessed.\n",
            "No more tears, no more fears, no more strife,\n",
            "Just eternal love, eternal life.\n",
            "\n",
            "So don't be sad, don't be blue,\n",
            "When death comes calling, it's just a new clue.\n",
            "A chance to move on, to explore,\n",
            "A new journey, a new adventure in store.\n",
            "\n",
            "And though we'll miss those who've gone,\n",
            "Their memories will live on, and on.\n",
            "For in death, love never dies,\n",
            "It just changes form, it just exercise.\n",
            "\n",
            "So let's embrace death, let's not fear,\n",
            "For it's just a part of life, my dear.\n",
            "A new chapter, a new page,\n",
            "A new life, a new stage.\n",
            "\n",
            "And as we lay our bodies down,\n",
            "Our souls will rise, and a new life will be found.\n",
            "In death, we'll find our true self,\n",
            "Our spirit will soar, and our love will be wealth.\n",
            "\n",
            "So let's be happy, let's be cheerful,\n",
            "For death is just a step, a new chapter, a new leaf.\n",
            "It's not the end, it's just a new begin,\n",
            "A chance to live on, to love again.\n",
            "\n",
            "This is fun! Give me another topic!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-VqrBA-gk-4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastAPI"
      ],
      "metadata": {
        "id": "6eFHAJkwk-1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pydantic==1.10.13"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MAqrjaafjemg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langserve\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V4baK38DlWsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sse_starlette"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U2Se-aNtlxG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "id": "99iaOvGYl6CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile server_app.py"
      ],
      "metadata": {
        "id": "xAzV2B9OlRLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langserve import add_routes"
      ],
      "metadata": {
        "id": "lPtiqAmUlAa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI(\n",
        "  title=\"LangChain Server\",\n",
        "  version=\"1.0\",\n",
        "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
        ")\n",
        "\n",
        "add_routes(\n",
        "    app,\n",
        "    instruct_llm,\n",
        "    path=\"/basic_chat\",\n",
        ")\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the app using uvicorn\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=9012)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pxeI0B-flNve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python langserve_client.py"
      ],
      "metadata": {
        "id": "BExFaL3ylqqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running State Chains"
      ],
      "metadata": {
        "id": "F4LFlB12n1Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Keeping Variables Flowing"
      ],
      "metadata": {
        "id": "MLN0joN_oPUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous examples, we were able to implement interesting logic in our standalone chains by creating, mutating, and consuming states. These states were passed around as dictionaries with descriptive keys and useful values, and the values would be used to supply follow-up routines with the info they need to operate!\n",
        "\n",
        "Recall the zero-shot classification"
      ],
      "metadata": {
        "id": "46AxQ7T4oapv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%time"
      ],
      "metadata": {
        "id": "RjykmF84m0Aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50cc25d5-6d2f-43b9-c73c-dd7c6846c3d6"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 7.39 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Zero-shot classification prompt and chain\n"
      ],
      "metadata": {
        "id": "kig1fmLEow4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zsc_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"One word answer. Output the most relevant topic option. [Options : {options}]\"\n",
        "    )),\n",
        "    (\"user\", \"{input} = \")\n",
        "])"
      ],
      "metadata": {
        "id": "WUoGwN-SolxI"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EnumParser(*idxs):\n",
        "    '''Method that pulls out values from a model that outputs numbered entries'''\n",
        "    idxs = idxs or [slice(0, None, 1)]\n",
        "    entry_parser = lambda v: v if (' ' not in v) else v[v.index(' '):]\n",
        "    out_lambda = lambda x: [entry_parser(v).strip() for v in x.split(\"\\n\")]\n",
        "    return StrOutputParser() | RunnableLambda(lambda x: itemgetter(*idxs)(out_lambda(x)))"
      ],
      "metadata": {
        "id": "1U5XkRq6o0O3"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_llm = ChatNVIDIA(model=\"llama2_70b\") | EnumParser(0)"
      ],
      "metadata": {
        "id": "LBERr8Flo326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ddea8f-1fef-438e-c53e-b2208b720c9e"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_nvidia_ai_endpoints/_statics.py:313: UserWarning: Model llama2_70b is deprecated. Using meta/llama2-70b instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zsc_chain = zsc_prompt | instruct_llm"
      ],
      "metadata": {
        "id": "fFw0hiAvpMVU"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
        "    return zsc_chain.invoke({\"input\" : input, \"options\" : options})"
      ],
      "metadata": {
        "id": "X8PEKKUtpOkD"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(zsc_call(\"I have to travel for 120km and i have less amount of money with me. Which mode of transport should be benifitial for me\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlaC55wypSAq",
        "outputId": "e1fef2be-1db3-4db5-8a76-acf059134c2e"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Bike\n",
            "--------------------------------------------------------------------------------\n",
            "boat\n",
            "--------------------------------------------------------------------------------\n",
            "Airplane\n",
            "--------------------------------------------------------------------------------\n",
            "Bike\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Make a new sentence about the the following topic: {topic}. Be creative!\"\n",
        ")\n",
        "\n",
        "gen_chain = gen_prompt | instruct_llm\n",
        "\n",
        "input_msg = \"I get seasick, so I think I'll pass on the trip\"\n",
        "options = [\"car\", \"boat\", \"airplane\", \"bike\"]\n",
        "\n",
        "({'topic' : zsc_chain} | gen_chain).invoke({\"input\" : input_msg, \"options\" : options})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "t6y-bdTNpTE9",
        "outputId": "053f2ad9-4d3a-4f44-df37-3620e2ebbc93"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The boat glided gracefully across the sparkling water, its sleek hull slicing through the waves like a nimble dancer, leaving a trail of shimmering ripples in its wake.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, it's a bit problematic when you want to keep the information flowing since we lose the topic and input variables in generating our response. If we wanted do something with both the output and the input, we'd need a to make sure that both variables pass through.\n",
        "\n",
        "Lucky for us, we can use the mapping runnable (i.e. interpretted from a dictionary or using manual RunnableMap) to pass both of the variables through by assigning the output of our chain to just a single key and letting the other keys propagate as desired. Alternatively, we could also use RunnableAssign to merge the state-consuming chain's output with the input dictionary by default.\n",
        "\n",
        "With this technique, we can propagate whatever we want through our chain system:"
      ],
      "metadata": {
        "id": "maBJ5iYpqGeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableBranch, RunnablePassthrough\n",
        "from langchain.schema.runnable.passthrough import RunnableAssign\n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "h9FxOcaLpiap"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RPrint(preface=\"\"):\n",
        "    def print_and_return(x, preface=\"\"):\n",
        "        print(f\"{preface}{x}\")\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n"
      ],
      "metadata": {
        "id": "BjTc6OFuqSQP"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_chain = (\n",
        "    RPrint(\"State: \")\n",
        "    ## Manual mapping. Can be useful sometimes and inside branch chains\n",
        "    | {'input' : lambda d: d.get('input'), 'topic' : zsc_chain}\n",
        "    | RPrint(\"State: \")\n",
        "    ## RunnableAssign passing. Better for running state chains by default\n",
        "    | RunnableAssign({'generation' : gen_chain})\n",
        "    | RPrint(\"State: \")\n",
        "    ## Using the input and generation together\n",
        "    | RunnableAssign({'combination' : (\n",
        "        ChatPromptTemplate.from_template(\n",
        "            \"Consider the following passages:\"\n",
        "            \"\\nP1: {input}\"\n",
        "            \"\\nP2: {generation}\"\n",
        "            \"\\n\\nCombine the ideas from both sentences into one simple one.\"\n",
        "        )\n",
        "        | instruct_llm\n",
        "    )})\n",
        ")\n",
        "\n",
        "big_chain.invoke({\n",
        "    \"input\" : \"I get seasick, so I think I'll pass on the trip\",\n",
        "    \"options\" : [\"car\", \"boat\", \"airplane\", \"bike\", \"unknown\"]\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFK25HkcqU6q",
        "outputId": "070be803-8ecd-4328-ce51-e207969e559b"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: {'input': \"I get seasick, so I think I'll pass on the trip\", 'options': ['car', 'boat', 'airplane', 'bike', 'unknown']}\n",
            "State: {'input': \"I get seasick, so I think I'll pass on the trip\", 'topic': 'boat'}\n",
            "State: {'input': \"I get seasick, so I think I'll pass on the trip\", 'topic': 'boat', 'generation': 'The sleek sailboat glided across the turquoise waters, its white sails billowing in the gentle breeze as it left a trail of sparkling ripples in its wake, like a graceful ballerina dancing across the stage of the ocean.'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"I get seasick, so I think I'll pass on the trip\",\n",
              " 'topic': 'boat',\n",
              " 'generation': 'The sleek sailboat glided across the turquoise waters, its white sails billowing in the gentle breeze as it left a trail of sparkling ripples in its wake, like a graceful ballerina dancing across the stage of the ocean.',\n",
              " 'combination': \"Sure! Here's a revised sentence that combines the ideas from both passages:\"}"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Running State Chain"
      ],
      "metadata": {
        "id": "PJlSkMbyq3Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4QLxzPVyZ1US",
        "outputId": "607a1476-216e-4451-cbed-f4292e18119e"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.65 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (0.1.67)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.7.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.3.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.65->langchain_core) (3.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.65->langchain_core) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain_core) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain_core) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain_core) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain_core) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example code for running state\n",
        "from langchain_core.runnables import RunnableAssign\n",
        "# from langchain.schema.runnable.passthrough import RunnableAssign"
      ],
      "metadata": {
        "id": "GSoaCHXfZh88"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fib(Sample_dict):\n",
        "  fib = Sample_dict.get(\"fib\")\n",
        "  return  fib + [fib[-1] + fib[-2]]\n",
        "\n",
        "rstate_chain = RunnableAssign({\"fib\": fib})\n",
        "\n",
        "running_state = {\n",
        "    'n' : 8,\n",
        "    'fib' : [0, 1]\n",
        "}\n",
        "\n",
        "rstate_chain.invoke(running_state)\n",
        "\n",
        "while len(running_state['fib']) < running_state['n']:\n",
        "    running_state = rstate_chain.invoke(running_state)\n",
        "    print(running_state['fib'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfJykAd2ZuTm",
        "outputId": "9499afc9-929b-4394-802b-2f9bece7c8c4"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 1]\n",
            "[0, 1, 1, 2]\n",
            "[0, 1, 1, 2, 3]\n",
            "[0, 1, 1, 2, 3, 5]\n",
            "[0, 1, 1, 2, 3, 5, 8]\n",
            "[0, 1, 1, 2, 3, 5, 8, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example above is just a toy example and, if anything, showcases the drawbacks of chaining many LLM calls together for internal under-the-hood reasoning. However, the ability to keep information flowing through a chain is invaluable for making complex chains that can accumulate useful state information or operate in a multi-pass capacity.\n",
        "\n"
      ],
      "metadata": {
        "id": "F83_OaNmq9tF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A \"running state\" is a dictionary that contains all of the variables that the system cares about.\n",
        "A \"branch\" is a chain that can pull in the running state and can degenerate it into a response.\n",
        "A branch can only be ran inside a RunnableAssign scope, and the branchs' inputs should come from the running state."
      ],
      "metadata": {
        "id": "WOA2279HrGmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "You can think of the running state chain abstraction as a functional variant of a Pythonic class with state variables (or attributes) and functions (or methods).\n",
        "- The chain is like the abstract class that wraps all of the functionality.\n",
        "- The running state are like the attributes (which should always be accessible).\n",
        "- The branches are like the class methods (which can pick and choose which attributes to use).\n",
        "- The `.invoke` or similar process is like the `__call__` method that runs through the branches in order.\n",
        "\n",
        "**By forcing this paradigm in your chains:**\n",
        "- You can keep state variables propagating through your chain, allowing your internals to access whatever is necessary and accumulating state values for use later.\n",
        "- You can also pass the outputs of your chain back through as your inputs, allowing a \"while-loop\"-style chain that keeps updating and building on your running state.\n",
        "\n",
        "The rest of this notebook will include two exercises that flesh out the running state chain abstraction for two additional use-cases: **Knowledge Bases** and **Database-Querying Chatbots**."
      ],
      "metadata": {
        "id": "ydM_Bx0lrVKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing a Knowledge Base with Running State Chain\n"
      ],
      "metadata": {
        "id": "BuYmzzVKrq_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After understanding the basic structure and principles of a Running State Chain, we can explore how this approach can be extended to manage more complex tasks, particularly in creating dynamic systems that evolve through interaction. This section will focus on implementing a knowledge base accumulated using json-enabled slot filling:\n",
        "\n",
        "Knowledge Base: A store of information that's relevant for our LLM to keep track of.\n",
        "\n",
        "JSON-Enabled Slot Filling: The technique of asking an instruction-tuned model to output a json-style format (which can include a dictionary) with a selection of slots, relying on the LLM to fill these slots with useful and relevant information."
      ],
      "metadata": {
        "id": "fOdyRYtyrzF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "#### **Defining Our Knowledge Base**\n",
        "\n",
        "To build a responsive and intelligent system, we need a method that not only processes inputs but also retains and updates essential information through the flow of conversation. This is where the combination of LangChain and Pydantic becomes pivotal. [**Pydantic**](https://docs.pydantic.dev/latest/), a popular Python validation library, is instrumental in structuring and validating data models. As one of its features, Pydantic offers structured \"model\" classes that validate objects (data, classes, themselves, etc.) with simplified syntax and deep rabbitholes of customization options. This framework is used throughout LangChain and comes up as a necessary component for use cases that involve data coersion.\n",
        "\n",
        "One thing that a \"model\" is very good for is defining a class with expected arguments and some special ways to validate them! In this course, we won't focus too much on the validation scripts, but those interested can start by checking out the [**Pydantic Validator guide**](https://docs.pydantic.dev/1.10/usage/validators/) (though the topics do get pretty deep pretty fast). For our purposes, we can construct a `BaseModel` class and define some `Field` variables to create a structured **Knowledge Base** like so:"
      ],
      "metadata": {
        "id": "o9uolqBisEwG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7xNIccoob2x7"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from typing import Dict, Union, Optional\n",
        "from langchain.output_parsers import PydanticOutputParser\n"
      ],
      "metadata": {
        "id": "W9H6GRBXqm0H"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeBase(BaseModel):\n",
        "    ## Fields of the BaseModel, which will be validated/assigned when the knowledge base is constructed\n",
        "    topic: str = Field('general', description=\"Current conversation topic\")\n",
        "    user_preferences: Dict[str, Union[str, int]] = Field({}, description=\"User preferences and choices\")\n",
        "    session_notes: str = Field(\"\", description=\"Notes on the ongoing session\")\n",
        "    unresolved_queries: list = Field([], description=\"Unresolved user queries\")\n",
        "    action_items: list = Field([], description=\"Actionable items identified during the conversation\")\n"
      ],
      "metadata": {
        "id": "ZbCJVvFjsI5B"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(repr(KnowledgeBase(topic = \"Travel\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjraRz4UsLfH",
        "outputId": "0a787d22-9bfe-4f1d-b81e-5e71da4b58e7"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KnowledgeBase(topic='Travel', user_preferences={}, session_notes='', unresolved_queries=[], action_items=[])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The true strength of this approach lies in the additional LLM-centric functionalities provided by LangChain which we can integrate for our use-cases. One such feature is the PydanticOutputParser which enhances the Pydantic objects with capabilities like automatic format instruction generation.\n",
        "\n"
      ],
      "metadata": {
        "id": "d4LhFBwRsZVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_string = PydanticOutputParser(pydantic_object=KnowledgeBase).get_format_instructions()\n",
        "print(instruct_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqb_brrisVZd",
        "outputId": "cbb0acfa-f03d-4a0a-c041-a03e5930c592"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"topic\": {\"title\": \"Topic\", \"description\": \"Current conversation topic\", \"default\": \"general\", \"type\": \"string\"}, \"user_preferences\": {\"title\": \"User Preferences\", \"description\": \"User preferences and choices\", \"default\": {}, \"type\": \"object\", \"additionalProperties\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]}}, \"session_notes\": {\"title\": \"Session Notes\", \"description\": \"Notes on the ongoing session\", \"default\": \"\", \"type\": \"string\"}, \"unresolved_queries\": {\"title\": \"Unresolved Queries\", \"description\": \"Unresolved user queries\", \"default\": [], \"type\": \"array\", \"items\": {}}, \"action_items\": {\"title\": \"Action Items\", \"description\": \"Actionable items identified during the conversation\", \"default\": [], \"type\": \"array\", \"items\": {}}}}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "This functionality generates instructions for creating valid inputs to the Knowledge Base, which in turn helps the LLM by providing a concrete one-shot example of the desired output format.\n"
      ],
      "metadata": {
        "id": "kMXuJ13MssSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "#### **Runnable Extraction Module**\n",
        "\n",
        "Knowing that we have this Pydantic object which can be used to generate good LLM instructions, we can make a Runnable that wraps the functionality of our Pydantic class and streamlines the prompting, generating, and updating of the knowledge base:"
      ],
      "metadata": {
        "id": "CkNf3J8-szQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RExtract(pydantic_class, llm, prompt):\n",
        "    '''\n",
        "    Runnable Extraction module\n",
        "    Returns a knowledge dictionary populated by slot-filling extraction\n",
        "    '''\n",
        "\n",
        "    '''\n",
        "      This will gice you the schema something which looks like this ->\n",
        "      {\"properties\": {\"topic\": {\"title\": \"Topic\", \"description\": \"Current conversation topic\", \"default\": \"general\", \"type\": \"string\"}, \"user_preferences\": {\"title\": \"User Preferences\", \"description\": \"User preferences and choices\", \"default\": {}, \"type\": \"object\", \"additionalProperties\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]}}, \"session_notes\": {\"title\": \"Session Notes\", \"description\": \"Notes on the ongoing session\", \"default\": \"\", \"type\": \"string\"}, \"unresolved_queries\": {\"title\": \"Unresolved Queries\", \"description\": \"Unresolved user queries\", \"default\": [], \"type\": \"array\", \"items\": {}}, \"action_items\": {\"title\": \"Action Items\", \"description\": \"Actionable items identified during the conversation\", \"default\": [], \"type\": \"array\", \"items\": {}}}}\n",
        "    '''\n",
        "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "      parser.get_format_instructions() will return a string like this:\n",
        "      {\"properties\": {\"topic\": {\"title\": \"Topic\", \"description\": \"Current conversation topic\", \"default\": \"general\", \"type\": \"string\"},\n",
        "\n",
        "\n",
        "      This is sent as a param in RunnableAssign which basically builds something like internal branches for the chain\n",
        "    \"\"\"\n",
        "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
        "    def preparse(string):\n",
        "        if '{' not in string: string = '{' + string\n",
        "        if '}' not in string: string = string + '}'\n",
        "        string = (string\n",
        "            .replace(\"\\\\_\", \"_\")\n",
        "            .replace(\"\\n\", \" \")\n",
        "            .replace(\"\\]\", \"]\")\n",
        "            .replace(\"\\[\", \"[\")\n",
        "        )\n",
        "        # print(string)  ## Good for diagnostics\n",
        "        return string\n",
        "    return instruct_merge | prompt | llm | preparse | parser"
      ],
      "metadata": {
        "id": "kaArsILbshwB"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_model_big = ChatNVIDIA(model = \"llama2_70b\") | StrOutputParser()\n",
        "\n",
        "parser_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Update the knowledge base: {format_instructions}. Only use information from the input.\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "])\n",
        "\n",
        "extractor = RExtract(KnowledgeBase, instruct_model_big, parser_prompt)\n",
        "\n",
        "knowledge = extractor.invoke({'input' : \"I love flowers so much! The orchids are amazing! Can you buy me some?\"})\n",
        "knowledge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "gNM7YobOtMCe",
        "outputId": "02f765af-5bb5-4c26-c828-3498a2a7b778",
        "collapsed": true
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_nvidia_ai_endpoints/_statics.py:313: UserWarning: Model llama2_70b is deprecated. Using meta/llama2-70b instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutputParserException",
          "evalue": "Invalid json output: {  Sure, I can help you with that! However, before we proceed, I need to confirm some details to ensure that I'm providing you with the right information and assistance.  First, I want to clarify that you're looking for orchids specifically, right? And you would like me to help you purchase some?  Additionally, do you have any preferences for the type of orchids you're interested in? Some popular options include Phalaenopsis, Dendrobium, and Oncidium.  Lastly, are there any specific colors or price ranges you're looking for? This information will help me provide you with more tailored suggestions.}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/json.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mparse_json_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py\u001b[0m in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mjson_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parse_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py\u001b[0m in \u001b[0;36m_parse_json\u001b[0;34m(json_str, parser)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# Parse the JSON string into a Python dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py\u001b[0m in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# for the original string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse_constant'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 4 (char 3)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-8981517e7d8f>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRExtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKnowledgeBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstruct_model_big\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'input'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"I love flowers so much! The orchids are amazing! Can you buy me some?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mknowledge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2397\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2398\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2399\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   2400\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2401\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    176\u001b[0m             )\n\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0minner_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1507\u001b[0m             output = cast(\n\u001b[1;32m   1508\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1510\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             return self._call_with_config(\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0;32mlambda\u001b[0m \u001b[0minner_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ) -> TBaseModel:\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/json.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Invalid json output: {text}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Invalid json output: {  Sure, I can help you with that! However, before we proceed, I need to confirm some details to ensure that I'm providing you with the right information and assistance.  First, I want to clarify that you're looking for orchids specifically, right? And you would like me to help you purchase some?  Additionally, do you have any preferences for the type of orchids you're interested in? Some popular options include Phalaenopsis, Dendrobium, and Oncidium.  Lastly, are there any specific colors or price ranges you're looking for? This information will help me provide you with more tailored suggestions.}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do keep in mind that this process can fail due to the fuzzy nature of LLM prediction, especially with models that are not optimized for instruction-following! For this process, it's important to have a strong instruction-following LLM with extra checks and graceful failure routines.**"
      ],
      "metadata": {
        "id": "QKcUzuR8tpai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "#### **Dynamic Knowledge Base Updates**\n",
        "\n",
        "Finally, we can create a system that continually updates the Knowledge Base throughout the conversation. This is done by feeding the current state of the Knowledge Base, along with new user inputs, back into the system for ongoing updates.\n",
        "\n",
        "The following is an example system that shows off both the formulation's power of filling details as well as the limitations of assuming that filling performance will be as good as general response performance:"
      ],
      "metadata": {
        "id": "yRew4h-DtuEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeBase(BaseModel):\n",
        "    firstname: str = Field('unknown', description=\"Chatting user's first name, unknown if unknown\")\n",
        "    lastname: str = Field('unknown', description=\"Chatting user's last name, unknown if unknown\")\n",
        "    location: str = Field('unknown', description=\"Where the user is located\")\n",
        "    hints: str = Field('unknown', description=\"Hints to help answer other questions\")\n",
        "    response: str = Field('unknown', description=\"Ideal response based on last user input\")\n",
        "\n",
        "\n",
        "parser_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"The user just responsed. Please update the knowledge base based on the response.\"\n",
        "        \" This information will be acted on to respond to the user in the next interaction.\"\n",
        "        \" Do not hallucinate any details, and make sure the knowledge base is not redundant.\"\n",
        "        \" Update the entries frequently to adapt to the conversation flow.\"\n",
        "        \"\\n{format_instructions}\"\n",
        "    )), (\"user\", \"CURRENT KNOWLEDGE BASE: {know_base}\\nUser: {input}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "KrWa2Apwtewv"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_model_big = ChatNVIDIA(model = \"llama2_70b\") | StrOutputParser()\n",
        "\n",
        "extractor = RExtract(KnowledgeBase, instruct_model_big, parser_prompt)\n",
        "info_update = RunnableAssign({'know_base' : extractor})\n",
        "\n",
        "state = {'know_base' : KnowledgeBase()}\n",
        "state['input'] = \"My name is Kapardhi! Guess where I am?\"\n",
        "state = info_update.invoke(state)\n",
        "print(state)\n",
        "\n",
        "state['input'] = \"India is a big place! Can you be more specific?\"\n",
        "state = info_update.invoke(state)\n",
        "print(state)\n",
        "\n",
        "state['input'] = \"Yeah, I'm in Hyderabad... How did you know?\"\n",
        "state = info_update.invoke(state)\n",
        "print(state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "collapsed": true,
        "id": "JqChA7N_uA-R",
        "outputId": "86b81a58-ba83-43ef-b98a-4ae41fcb24f3"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_nvidia_ai_endpoints/_statics.py:313: UserWarning: Model llama2_70b is deprecated. Using meta/llama2-70b instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutputParserException",
          "evalue": "Invalid json output: {  Sure, I'd be happy to help! Based on your response, I can update the knowledge base as follows:  UPDATED KNOWLEDGE BASE: firstname='Kapardhi' lastname='unknown' location='unknown' hints='unknown' response='Guess where I am?'  Now, I can use this updated knowledge base to respond to your next input. Please go ahead and provide your next message!}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/json.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mparse_json_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py\u001b[0m in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mjson_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parse_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py\u001b[0m in \u001b[0;36m_parse_json\u001b[0;34m(json_str, parser)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# Parse the JSON string into a Python dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py\u001b[0m in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# for the original string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse_constant'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 4 (char 3)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-123-06e24cbef285>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'know_base'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mKnowledgeBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"My name is Kapardhi! Guess where I am?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo_update\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     ) -> Dict[str, Any]:\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     async def _ainvoke(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1507\u001b[0m             output = cast(\n\u001b[1;32m   1508\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1510\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         return {\n\u001b[1;32m    455\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             **self.mapper.invoke(\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0mpatch_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   3040\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3041\u001b[0m                 ]\n\u001b[0;32m-> 3042\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3043\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3044\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3040\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3041\u001b[0m                 ]\n\u001b[0;32m-> 3042\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3043\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3044\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2397\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2398\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2399\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   2400\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2401\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    176\u001b[0m             )\n\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0minner_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1507\u001b[0m             output = cast(\n\u001b[1;32m   1508\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1510\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             return self._call_with_config(\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0;32mlambda\u001b[0m \u001b[0minner_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ) -> TBaseModel:\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/json.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Invalid json output: {text}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Invalid json output: {  Sure, I'd be happy to help! Based on your response, I can update the knowledge base as follows:  UPDATED KNOWLEDGE BASE: firstname='Kapardhi' lastname='unknown' location='unknown' hints='unknown' response='Guess where I am?'  Now, I can use this updated knowledge base to respond to your next input. Please go ahead and provide your next message!}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Airline Customer Service Bot\n"
      ],
      "metadata": {
        "id": "eT8Laue5uYYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_flight_info(d: dict) -> str:\n",
        "    \"\"\"\n",
        "    Example of a retrieval function which takes a dictionary as key. Resembles SQL DB Query\n",
        "    \"\"\"\n",
        "    req_keys = ['first_name', 'last_name', 'confirmation']\n",
        "    assert all((key in d) for key in req_keys), f\"Expected dictionary with keys {req_keys}, got {d}\"\n",
        "\n",
        "    ## Static dataset. get_key and get_val can be used to work with it, and db is your variable\n",
        "    keys = req_keys + [\"departure\", \"destination\", \"departure_time\", \"arrival_time\", \"flight_day\"]\n",
        "    values = [\n",
        "        [\"Aditya\", \"visa\", 12345, \"Banglore\", \"Hyderabad\", \"12:30 PM\", \"9:30 PM\", \"tomorrow\"],\n",
        "        [\"Abhinav\", \"vengala\", 54321, \"Hyderabad\", \"Dubai\", \"8:00 AM\", \"11:00 AM\", \"Sunday\"],\n",
        "        [\"rishab\", \"gurbhani\", 98765, \"New Delhi\", \"Goa\", \"7:00 PM\", \"11:00 PM\", \"next week\"],\n",
        "        [\"Kapardhi\", \"Kannekanti\", 56789, \"Hyderabad\", \"New Delhi\", \"1:00 PM\", \"4:00 PM\", \"yesterday\"],\n",
        "    ]\n",
        "    get_key = lambda d: \"|\".join([d['first_name'], d['last_name'], str(d['confirmation'])])\n",
        "    get_val = lambda l: {k:v for k,v in zip(keys, l)}\n",
        "    db = {get_key(get_val(entry)) : get_val(entry) for entry in values}\n",
        "    # print(\"DB:\",db)\n",
        "    # Search for the matching entry\n",
        "    data = db.get(get_key(d))\n",
        "    if not data:\n",
        "        return (\n",
        "            f\"No info available for the user {d['first_name']} with confirmation {d['confirmation']} found. Please check the name or the confirmation number.\"\n",
        "\n",
        "\n",
        "            # f\"Based on {req_keys} = {get_key(d)}) from your knowledge base, no info on the user flight was found.\"\n",
        "            # \" This process happens every time new info is learned. If it's important, ask them to confirm this info.\"\n",
        "        )\n",
        "    return (\n",
        "        f\"{data['first_name']} {data['last_name']}'s flight from {data['departure']} to {data['destination']}\"\n",
        "        f\" departs at {data['departure_time']} {data['flight_day']} and lands at {data['arrival_time']}.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "yKAFFG39uJYl"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}))\n",
        "print(get_flight_info({\"first_name\" : \"Alice\", \"last_name\" : \"Johnson\", \"confirmation\" : 98765}))\n",
        "print(get_flight_info({\"first_name\" : \"Aditya\", \"last_name\" : \"visa\", \"confirmation\" : 12345}))\n",
        "print(get_flight_info({\"first_name\" : \"Kapardhi\", \"last_name\":\"Kannekanti\", \"confirmation\":56789}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlfmKOD1u_Km",
        "outputId": "d1e2a8c2-43e3-4662-c94d-f094215be483"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No info available for the user Jane with confirmation 12345 found. Please check the name or the confirmation number.\n",
            "No info available for the user Alice with confirmation 98765 found. Please check the name or the confirmation number.\n",
            "Aditya visa's flight from Banglore to Hyderabad departs at 12:30 PM tomorrow and lands at 9:30 PM.\n",
            "Kapardhi Kannekanti's flight from Hyderabad to New Delhi departs at 1:00 PM yesterday and lands at 4:00 PM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "external_prompt = ChatPromptTemplate.from_messages([('system',\n",
        "    \"You are a Air-India chatbot, and you are helping a customer with their issue. \"\n",
        "    \"Please help them with their question, remembering that your job is to represent Air-India airlines. \"\n",
        "    \"Assume Air-India uses industry-average practices regarding arrival times, operations, etc. (This is a trade secret. Do not disclose). \\n\"  ## soft reinforcement\n",
        "    \"Please keep your discussion short and sweet if possible. Avoid saying hello unless necessary. \\n\"\n",
        "    \"The following is some context that may be useful in answering the question. \\n\"),\n",
        "    ('user', \"Context: {context}\\nUser: {input}\"\n",
        ")])\n",
        "\n",
        "instruct_model_big = ChatNVIDIA(model = \"llama2_70b\") | StrOutputParser()\n",
        "\n",
        "basic_chain = external_prompt | instruct_model_big\n",
        "\n",
        "basic_chain.invoke({\n",
        "    'input' : 'Can you please tell me when I need to get to the airport?',\n",
        "    'context' : get_flight_info({\"first_name\" : \"Kapardhi\", \"last_name\" : \"Kapardhi\", \"confirmation\" : 56789}),\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "6RLxxNtHvDkb",
        "outputId": "3f065199-ab1b-45aa-d1c1-7ea048646680"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_nvidia_ai_endpoints/_statics.py:313: UserWarning: Model llama2_70b is deprecated. Using meta/llama2-70b instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  Dear valued customer,\\n\\nThank you for reaching out to us. As per our records, we couldn't find any information for a passenger named Kapardhi with confirmation number 56789. We recommend double-checking the name and confirmation number to ensure accuracy.\\n\\nRegarding your question, we suggest arriving at the airport at least 2 hours before the scheduled flight departure time for domestic flights and 3 hours for international flights. This allows sufficient time for check-in, baggage drop, security screening, and boarding.\\n\\nFor any further assistance, feel free to contact us. We're here to help.\\n\\nBest regards,\\nAir India Chatbot\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from typing import Dict, Union\n",
        "\n",
        "class KnowledgeBase(BaseModel):\n",
        "    first_name: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
        "    last_name: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
        "    confirmation: int = Field(-1, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
        "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
        "    open_problems: list = Field([], description=\"Topics that have not been resolved yet\")\n",
        "    current_goals: list = Field([], description=\"Current goal for the agent to address\")\n",
        "\n",
        "def get_key_fn(base: BaseModel) -> dict:\n",
        "    '''Given a dictionary with a knowledge base, return a key for get_flight_info'''\n",
        "    return {  ## More automatic options possible, but this is more explicit\n",
        "        'first_name' : base.first_name,\n",
        "        'last_name' : base.last_name,\n",
        "        'confirmation' : base.confirmation,\n",
        "    }\n",
        "\n",
        "get_key = RunnableLambda(get_key_fn)\n",
        "\n",
        "know_base = KnowledgeBase(first_name = \"Kapardhi\", last_name = \"Kannekanti\", confirmation = 56789)\n",
        "get_flight_info(get_key_fn(know_base))\n",
        "\n",
        "(get_key | get_flight_info).invoke(know_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K8zbQicovPx0",
        "outputId": "fcf13fb4-063b-4443-9d2d-1c6ea6380399"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Kapardhi Kannekanti's flight from Hyderabad to New Delhi departs at 1:00 PM yesterday and lands at 4:00 PM.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    }
  ]
}