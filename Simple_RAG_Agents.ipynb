{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG9J0Sd2O4KN",
        "outputId": "bfc2dbc4-54ad-474c-af02-480fa6623c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "7fMNDs3HQYl4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import requests\n",
        "import os\n",
        "\n",
        "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
        "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
        "    ## Try to set NVIDIA_API_KEY as part of docker_router routine.\n",
        "    ##  When running in course container, this helps to save your API key between sessions.\n",
        "    try:\n",
        "        assert not hard_reset\n",
        "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
        "        assert response.get('nvapi_key')\n",
        "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
        "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
        "    except: pass\n",
        "    hard_reset = False\n",
        "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
        "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
        "\n",
        "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q93zM68JRI3Z",
        "outputId": "8c3ee986-fe88-493d-b907-9c94bb0bba5e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA API Key: ··········\n",
            "Retrieved NVIDIA_API_KEY beginning with \"nvapi-F82...\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY')}\",\n",
        "    \"accept\": \"text/event-stream\",\n",
        "    \"content-type\": \"application/json\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "P_iD2rqVQLTE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stream_token(entry: bytes):\n",
        "    \"\"\"Utility: Coerces out ['choices'][0]['delta'][content] from the bytestream\"\"\"\n",
        "    if not entry: return \"\"\n",
        "    entry = entry.decode('utf-8')\n",
        "    if entry.startswith('data: '):\n",
        "        try: entry = json.loads(entry[5:])\n",
        "        except ValueError: return \"\"\n",
        "    return entry.get('choices', [{}])[0].get('delta', {}).get('content')\n"
      ],
      "metadata": {
        "id": "CcLljM8dQJMt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "invoke_url = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/404\""
      ],
      "metadata": {
        "id": "nG8z9Qu0RuXn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "payload = {\n",
        "    \"prompt\": \"This is the input text or prompt for the language model.\",\n",
        "    \"max_tokens\": 100,\n",
        "    \"temperature\": 0.7,\n",
        "    \"stop\": [\"###\"]\n",
        "}"
      ],
      "metadata": {
        "id": "GPE2Fi2aR2eH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.post(invoke_url, headers=headers, json=payload, stream=True)"
      ],
      "metadata": {
        "id": "CntS3dVKTQTU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.raise_for_status()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "collapsed": true,
        "id": "eqYc3gRMTXvW",
        "outputId": "341f70a3-6766-43f1-cd71-227a82a1ddb9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "400 Client Error: Bad Request for url: https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/404",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-98371c55c61f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/404"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for line in response.iter_lines():\n",
        "    print(get_stream_token(line), end=\"\")\n",
        "    if line: print(line.decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "collapsed": true,
        "id": "niq6zPEuTb24",
        "outputId": "2048c6e9-9e11-44fa-d3b1-dd959ee08afc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'get'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f805df5bce7b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_stream_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f5796a1afdc6>\u001b[0m in \u001b[0;36mget_stream_token\u001b[0;34m(entry)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ChatNVIDIA"
      ],
      "metadata": {
        "id": "30cBA49LUg8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install pillow==10.0.1\n",
        "%pip install --upgrade imageio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCZr4HzsVVCj",
        "outputId": "83a375d0-2bd4-4cf1-ee1c-62c7614ad6a1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.6)\n",
            "Collecting imageio\n",
            "  Downloading imageio-2.34.1-py3-none-any.whl (313 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.25.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (9.4.0)\n",
            "Installing collected packages: imageio\n",
            "  Attempting uninstall: imageio\n",
            "    Found existing installation: imageio 2.31.6\n",
            "    Uninstalling imageio-2.31.6:\n",
            "      Successfully uninstalled imageio-2.31.6\n",
            "Successfully installed imageio-2.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q langchain-nvidia-ai-endpoints\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MONM0O6dTfEF",
        "outputId": "2c7aaa03-3676-40a4-d5d3-50e2fd51eb47"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_nvidia_ai_endpoints._common import NVEModel  ## Backend Model\n",
        "\n",
        "## Using the backbone NVIDIA Endpoints client, which makes the calls as you saw above\n",
        "NVEModel().available_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yHNVeMc5Uhy2",
        "outputId": "435aee3c-88ef-4423-89cb-b8b1904e8209"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ai-sdxl-turbo': 'f886140c-424e-4c82-a841-99e23f9ae35d',\n",
              " 'ai-microsoft-kosmos-2': '6018fed7-f227-48dc-99bc-3fd4264d5037',\n",
              " 'ai-mixtral-8x7b-instruct': 'a1e53ece-bff4-44d1-8b13-c009e5bf47f6',\n",
              " 'ai-gemma-7b': 'a13e3bed-ca42-48f8-b3f1-fbc47b9675f9',\n",
              " 'ai-codegemma-7b': '7dfc10a8-3cc4-448e-97c1-2213308dc222',\n",
              " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
              " 'ai-molmim-generate': '72be0b68-179f-412c-ac03-9a481f78cb9f',\n",
              " 'ai-diffdock': 'f3dda972-561a-4772-8c09-873594b6fb72',\n",
              " 'ai-codellama-70b': 'f6b06895-d073-4714-8bb2-26c09e9f6597',\n",
              " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
              " 'ai-arctic-embed-l': '1528a0ad-205a-46ac-a783-94e2372586a9',\n",
              " 'ai-llama3-8b': 'a5a3ad64-ec2c-4bfc-8ef7-5636f26630fe',\n",
              " 'ai-esmfold': 'a68c59e0-47a6-4a50-bf64-6d88766d56bf',\n",
              " 'ai-stable-diffusion-xl': 'c1b63bb0-448b-4e53-b2a7-fb0b3723cbe2',\n",
              " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
              " 'playground_mamba_chat': '381be320-4721-4664-bd75-58f8783b43c7',\n",
              " 'playground_llama2_code_70b': '2ae529dc-f728-4a46-9b8d-2697213666d8',\n",
              " 'playground_seamless': '72ad9555-2e3d-4e73-9050-a37129064743',\n",
              " 'playground_llama_guard': 'b34280ac-24e4-4081-bfaa-501e9ee16b6f',\n",
              " 'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520',\n",
              " 'playground_nemotron_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
              " 'playground_cuopt': '8f2fbd00-2633-41ce-ab4e-e5736d74bff7',\n",
              " 'ai-google-paligemma': 'a70e7356-c643-41b3-9a7e-b89ea1e7dea1',\n",
              " 'ai-neva-22b': 'bc205f8e-1740-40df-8d32-c4321763498a',\n",
              " 'ai-mistral-7b-instruct-v2': 'd7618e99-db93-4465-af4d-330213a7f51f',\n",
              " 'ai-seallm-7b': 'e71e675e-a53d-4f2b-a441-e1287bd17dc8',\n",
              " 'playground_phi2': '6251d6d2-54ee-4486-90f4-2792bf0d3acd',\n",
              " 'playground_kosmos_2': '0bcd1a8c-451f-4b12-b7f0-64b4781190d1',\n",
              " 'ai-embed-qa-4': '09c64e32-2b65-4892-a285-2f585408d118',\n",
              " 'ai-sea-lion-7b-instruct': '02f84bf4-c1a1-489b-a9de-ac3e8dcdec14',\n",
              " 'ai-stable-video-diffusion': '8cd594f1-6a4d-4f8f-82b4-d1bf89adae98',\n",
              " 'ai-google-deplot': '784a8ca4-ea7d-4c93-bb46-ec027c3fae47',\n",
              " 'ai-llama3-70b': 'a88f115a-4a47-4381-ad62-ca25dc33dc1b',\n",
              " 'playground_nv_llama2_rlhf_70b': '7b3e3361-4266-41c8-b312-f5e33c81fc92',\n",
              " 'ai-dbrx-instruct': '3d6c2ff8-8bfc-4d10-8fd0-b7337288e869',\n",
              " 'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
              " 'ai-rerank-qa-mistral-4b': '0bf77f50-5c35-4488-8e7a-f49bb1974af6',\n",
              " 'playground_yi_34b': '347fa3f3-d675-432c-b844-669ef8ee53df',\n",
              " 'ai-llama2-70b': '2fddadfb-7e76-4c8a-9b82-f7d3fab94471',\n",
              " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
              " 'ai-fuyu-8b': 'e598bfc1-b058-41af-869d-556d3c7e1b48',\n",
              " 'playground_smaug_72b': '008cff6d-4f4c-4514-b61e-bcfad6ba52a7',\n",
              " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158',\n",
              " 'playground_steerlm_llama_70b': 'd6fe6881-973a-4279-a0f8-e1d486c9618d',\n",
              " 'ai-vista-3d': '72311276-923f-4478-a506-d5b80914728a',\n",
              " 'ai-phi-3-mini': '4a58c6cb-a9b4-4014-99de-3e704d4ae687',\n",
              " 'ai-mixtral-8x22b': '39655fc1-9ebc-4b24-963e-6915ea6680de',\n",
              " 'ai-recurrentgemma-2b': '2f495340-a99f-4b4b-89bd-1beb003dd896',\n",
              " 'ai-mistral-large': '767b5b9a-3f9d-4c1d-86e8-fa861988cee7',\n",
              " 'ai-ai-weather-forecasting': '9cec444c-db1c-4525-9c6f-f40e4a5b11ce',\n",
              " 'playground_mistral_7b': '35ec3354-2681-4d0e-a8dd-80325dcf7c63',\n",
              " 'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e',\n",
              " 'playground_deplot': '3bc390c7-eeec-40f7-a64d-0c6a719985f7',\n",
              " 'playground_starcoder2_15b': '6acada03-fe2f-4e4d-9e0a-e711b9fd1b59',\n",
              " 'playground_nemotron_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
              " 'ai-mixtral-8x22b-instruct': '710c92d0-7c98-46d6-b5ae-07e84bcaa5d3',\n",
              " 'ai-parakeet-ctc-riva': '22164014-a6cc-4a6f-b048-f3a303e745bb',\n",
              " 'playground_mixtral_8x7b': '8f4118ba-60a8-4e6b-8574-e38a4067a4a3',\n",
              " 'playground_gemma_2b': '5bde8f6f-7e83-4413-a0f2-7b97be33988e',\n",
              " 'playground_gemma_7b': '1361fa56-61d7-4a12-af32-69a3825746fa',\n",
              " 'ai-arctic': '7408b6b5-09e7-4ae5-a3fe-2db063e4e609',\n",
              " 'ai-nvidia-cuopt': 'b0ac1378-3d00-43cb-a8d9-0f0c37ef36c0',\n",
              " 'ai-phi-3-mini-4k': 'ad974453-80d4-46df-a02d-6f7dae20c010',\n",
              " 'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
              " 'ai-gemma-2b': '04174188-f742-4069-9e72-d77c2b77d3cb',\n",
              " 'microsoft/kosmos-2': '6018fed7-f227-48dc-99bc-3fd4264d5037',\n",
              " 'mistralai/mixtral-8x7b-instruct-v0.1': 'a1e53ece-bff4-44d1-8b13-c009e5bf47f6',\n",
              " 'google/gemma-7b': 'a13e3bed-ca42-48f8-b3f1-fbc47b9675f9',\n",
              " 'google/codegemma-7b': '7dfc10a8-3cc4-448e-97c1-2213308dc222',\n",
              " 'meta/codellama-70b': 'f6b06895-d073-4714-8bb2-26c09e9f6597',\n",
              " 'snowflake/arctic-embed-l': '1528a0ad-205a-46ac-a783-94e2372586a9',\n",
              " 'meta/llama3-8b-instruct': 'a5a3ad64-ec2c-4bfc-8ef7-5636f26630fe',\n",
              " 'google/paligemma': 'a70e7356-c643-41b3-9a7e-b89ea1e7dea1',\n",
              " 'nvidia/neva-22b': 'bc205f8e-1740-40df-8d32-c4321763498a',\n",
              " 'mistralai/mistral-7b-instruct-v0.2': 'd7618e99-db93-4465-af4d-330213a7f51f',\n",
              " 'seallms/seallm-7b-v2.5': 'e71e675e-a53d-4f2b-a441-e1287bd17dc8',\n",
              " 'NV-Embed-QA': '09c64e32-2b65-4892-a285-2f585408d118',\n",
              " 'aisingapore/sea-lion-7b-instruct': '02f84bf4-c1a1-489b-a9de-ac3e8dcdec14',\n",
              " 'google/deplot': '784a8ca4-ea7d-4c93-bb46-ec027c3fae47',\n",
              " 'meta/llama3-70b-instruct': 'a88f115a-4a47-4381-ad62-ca25dc33dc1b',\n",
              " 'databricks/dbrx-instruct': '3d6c2ff8-8bfc-4d10-8fd0-b7337288e869',\n",
              " 'nv-rerank-qa-mistral-4b:1': '0bf77f50-5c35-4488-8e7a-f49bb1974af6',\n",
              " 'meta/llama2-70b': '2fddadfb-7e76-4c8a-9b82-f7d3fab94471',\n",
              " 'adept/fuyu-8b': 'e598bfc1-b058-41af-869d-556d3c7e1b48',\n",
              " 'microsoft/phi-3-mini-128k-instruct': '4a58c6cb-a9b4-4014-99de-3e704d4ae687',\n",
              " 'mistralai/mixtral-8x22b-v0.1': '39655fc1-9ebc-4b24-963e-6915ea6680de',\n",
              " 'google/recurrentgemma-2b': '2f495340-a99f-4b4b-89bd-1beb003dd896',\n",
              " 'mistralai/mistral-large': '767b5b9a-3f9d-4c1d-86e8-fa861988cee7',\n",
              " 'mistralai/mixtral-8x22b-instruct-v0.1': '710c92d0-7c98-46d6-b5ae-07e84bcaa5d3',\n",
              " 'snowflake/arctic': '7408b6b5-09e7-4ae5-a3fe-2db063e4e609',\n",
              " 'microsoft/phi-3-mini-4k-instruct': 'ad974453-80d4-46df-a02d-6f7dae20c010',\n",
              " 'google/gemma-2b': '04174188-f742-4069-9e72-d77c2b77d3cb'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "## NOTE: \"playground_\" prefix is optional for our client\n",
        "chat = ChatNVIDIA(model='llama2_70b')\n",
        "chat.invoke(\"Hello! How's it going?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beFjlu3VUxvK",
        "outputId": "9e1feef6-1f5e-417f-8b4b-34f444cafee3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatMessage(content=\"Hello! I'm doing well, thanks for asking. How about you? Is there anything you'd like to chat about or ask? I'm here to help with any questions you might have.\", response_metadata={'role': 'assistant', 'content': \"Hello! I'm doing well, thanks for asking. How about you? Is there anything you'd like to chat about or ask? I'm here to help with any questions you might have.\", 'token_usage': {'completion_tokens': 42, 'prompt_tokens': 16, 'total_tokens': 58}, 'model_name': 'llama2_70b'}, id='run-fb014708-b91c-460e-a8e8-46916f63291b-0', role='assistant')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AsuQOhLMVyzp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Environment Setup\n"
      ],
      "metadata": {
        "id": "excvgCYnYPex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q langchain langchain-nvidia-ai-endpoints gradio typing_extensions>=4.8.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjFCI1INYSDo",
        "outputId": "c396b9cb-00ea-47a2-b341-11a4f24bac38"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints._common import NVEModel"
      ],
      "metadata": {
        "id": "W-WKyhuWYTs_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import requests\n",
        "import os\n",
        "\n",
        "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
        "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
        "    try:\n",
        "        assert not hard_reset\n",
        "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
        "        assert response.get('nvapi_key')\n",
        "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
        "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
        "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
        "    except: pass\n",
        "    hard_reset = False\n",
        "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
        "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
        "\n",
        "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
        "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
        "NVEModel().available_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3j94egLYYaPZ",
        "outputId": "2693fcd2-9fe6-45a2-b1ca-ea7f4a813590"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved NVIDIA_API_KEY beginning with \"nvapi-F82...\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ai-parakeet-ctc-riva': '22164014-a6cc-4a6f-b048-f3a303e745bb',\n",
              " 'ai-llama2-70b': '2fddadfb-7e76-4c8a-9b82-f7d3fab94471',\n",
              " 'playground_gemma_7b': '1361fa56-61d7-4a12-af32-69a3825746fa',\n",
              " 'playground_llama2_code_70b': '2ae529dc-f728-4a46-9b8d-2697213666d8',\n",
              " 'playground_mistral_7b': '35ec3354-2681-4d0e-a8dd-80325dcf7c63',\n",
              " 'playground_yi_34b': '347fa3f3-d675-432c-b844-669ef8ee53df',\n",
              " 'playground_nemotron_qa_8b': '0c60f14d-46cb-465e-b994-227e1c3d5047',\n",
              " 'ai-sea-lion-7b-instruct': '02f84bf4-c1a1-489b-a9de-ac3e8dcdec14',\n",
              " 'playground_gemma_2b': '5bde8f6f-7e83-4413-a0f2-7b97be33988e',\n",
              " 'ai-molmim-generate': '72be0b68-179f-412c-ac03-9a481f78cb9f',\n",
              " 'ai-mixtral-8x22b-instruct': '710c92d0-7c98-46d6-b5ae-07e84bcaa5d3',\n",
              " 'ai-mistral-large': '767b5b9a-3f9d-4c1d-86e8-fa861988cee7',\n",
              " 'ai-microsoft-kosmos-2': '6018fed7-f227-48dc-99bc-3fd4264d5037',\n",
              " 'playground_seamless': '72ad9555-2e3d-4e73-9050-a37129064743',\n",
              " 'ai-arctic': '7408b6b5-09e7-4ae5-a3fe-2db063e4e609',\n",
              " 'playground_nv_llama2_rlhf_70b': '7b3e3361-4266-41c8-b312-f5e33c81fc92',\n",
              " 'ai-codegemma-7b': '7dfc10a8-3cc4-448e-97c1-2213308dc222',\n",
              " 'ai-mixtral-8x7b-instruct': 'a1e53ece-bff4-44d1-8b13-c009e5bf47f6',\n",
              " 'playground_mixtral_8x7b': '8f4118ba-60a8-4e6b-8574-e38a4067a4a3',\n",
              " 'playground_neva_22b': '8bf70738-59b9-4e5f-bc87-7ab4203be7a0',\n",
              " 'ai-rerank-qa-mistral-4b': '0bf77f50-5c35-4488-8e7a-f49bb1974af6',\n",
              " 'playground_fuyu_8b': '9f757064-657f-4c85-abd7-37a7a9b6ee11',\n",
              " 'ai-gemma-7b': 'a13e3bed-ca42-48f8-b3f1-fbc47b9675f9',\n",
              " 'playground_llama2_code_34b': 'df2bee43-fb69-42b9-9ee5-f4eabbeaf3a8',\n",
              " 'ai-google-deplot': '784a8ca4-ea7d-4c93-bb46-ec027c3fae47',\n",
              " 'ai-stable-diffusion-xl': 'c1b63bb0-448b-4e53-b2a7-fb0b3723cbe2',\n",
              " 'playground_kosmos_2': '0bcd1a8c-451f-4b12-b7f0-64b4781190d1',\n",
              " 'playground_llama_guard': 'b34280ac-24e4-4081-bfaa-501e9ee16b6f',\n",
              " 'ai-esmfold': 'a68c59e0-47a6-4a50-bf64-6d88766d56bf',\n",
              " 'playground_deplot': '3bc390c7-eeec-40f7-a64d-0c6a719985f7',\n",
              " 'playground_starcoder2_15b': '6acada03-fe2f-4e4d-9e0a-e711b9fd1b59',\n",
              " 'playground_sdxl': '89848fb8-549f-41bb-88cb-95d6597044a4',\n",
              " 'playground_cuopt': '8f2fbd00-2633-41ce-ab4e-e5736d74bff7',\n",
              " 'ai-mixtral-8x22b': '39655fc1-9ebc-4b24-963e-6915ea6680de',\n",
              " 'ai-llama3-8b': 'a5a3ad64-ec2c-4bfc-8ef7-5636f26630fe',\n",
              " 'ai-stable-video-diffusion': '8cd594f1-6a4d-4f8f-82b4-d1bf89adae98',\n",
              " 'ai-gemma-2b': '04174188-f742-4069-9e72-d77c2b77d3cb',\n",
              " 'ai-llama3-70b': 'a88f115a-4a47-4381-ad62-ca25dc33dc1b',\n",
              " 'ai-neva-22b': 'bc205f8e-1740-40df-8d32-c4321763498a',\n",
              " 'ai-dbrx-instruct': '3d6c2ff8-8bfc-4d10-8fd0-b7337288e869',\n",
              " 'ai-phi-3-mini-4k': 'ad974453-80d4-46df-a02d-6f7dae20c010',\n",
              " 'ai-nvidia-cuopt': 'b0ac1378-3d00-43cb-a8d9-0f0c37ef36c0',\n",
              " 'ai-vista-3d': '72311276-923f-4478-a506-d5b80914728a',\n",
              " 'playground_clip': '8c21289c-0b18-446d-8838-011b7249c513',\n",
              " 'playground_smaug_72b': '008cff6d-4f4c-4514-b61e-bcfad6ba52a7',\n",
              " 'ai-fuyu-8b': 'e598bfc1-b058-41af-869d-556d3c7e1b48',\n",
              " 'playground_steerlm_llama_70b': 'd6fe6881-973a-4279-a0f8-e1d486c9618d',\n",
              " 'playground_llama2_13b': 'e0bb7fb9-5333-4a27-8534-c6288f921d3f',\n",
              " 'ai-embed-qa-4': '09c64e32-2b65-4892-a285-2f585408d118',\n",
              " 'playground_phi2': '6251d6d2-54ee-4486-90f4-2792bf0d3acd',\n",
              " 'ai-seallm-7b': 'e71e675e-a53d-4f2b-a441-e1287bd17dc8',\n",
              " 'playground_llama2_70b': '0e349b44-440a-44e1-93e9-abe8dcb27158',\n",
              " 'ai-recurrentgemma-2b': '2f495340-a99f-4b4b-89bd-1beb003dd896',\n",
              " 'playground_mamba_chat': '381be320-4721-4664-bd75-58f8783b43c7',\n",
              " 'ai-mistral-7b-instruct-v2': 'd7618e99-db93-4465-af4d-330213a7f51f',\n",
              " 'ai-diffdock': 'f3dda972-561a-4772-8c09-873594b6fb72',\n",
              " 'ai-codellama-70b': 'f6b06895-d073-4714-8bb2-26c09e9f6597',\n",
              " 'ai-sdxl-turbo': 'f886140c-424e-4c82-a841-99e23f9ae35d',\n",
              " 'playground_nvolveqa_40k': '091a03bb-7364-4087-8090-bd71e9277520',\n",
              " 'ai-arctic-embed-l': '1528a0ad-205a-46ac-a783-94e2372586a9',\n",
              " 'ai-google-paligemma': 'a70e7356-c643-41b3-9a7e-b89ea1e7dea1',\n",
              " 'ai-ai-weather-forecasting': '9cec444c-db1c-4525-9c6f-f40e4a5b11ce',\n",
              " 'playground_nemotron_steerlm_8b': '1423ff2f-d1c7-4061-82a7-9e8c67afd43a',\n",
              " 'playground_llama2_code_13b': 'f6a96af4-8bf9-4294-96d6-d71aa787612e',\n",
              " 'ai-phi-3-mini': '4a58c6cb-a9b4-4014-99de-3e704d4ae687',\n",
              " 'meta/llama2-70b': '2fddadfb-7e76-4c8a-9b82-f7d3fab94471',\n",
              " 'aisingapore/sea-lion-7b-instruct': '02f84bf4-c1a1-489b-a9de-ac3e8dcdec14',\n",
              " 'mistralai/mixtral-8x22b-instruct-v0.1': '710c92d0-7c98-46d6-b5ae-07e84bcaa5d3',\n",
              " 'mistralai/mistral-large': '767b5b9a-3f9d-4c1d-86e8-fa861988cee7',\n",
              " 'microsoft/kosmos-2': '6018fed7-f227-48dc-99bc-3fd4264d5037',\n",
              " 'snowflake/arctic': '7408b6b5-09e7-4ae5-a3fe-2db063e4e609',\n",
              " 'google/codegemma-7b': '7dfc10a8-3cc4-448e-97c1-2213308dc222',\n",
              " 'mistralai/mixtral-8x7b-instruct-v0.1': 'a1e53ece-bff4-44d1-8b13-c009e5bf47f6',\n",
              " 'nv-rerank-qa-mistral-4b:1': '0bf77f50-5c35-4488-8e7a-f49bb1974af6',\n",
              " 'google/gemma-7b': 'a13e3bed-ca42-48f8-b3f1-fbc47b9675f9',\n",
              " 'google/deplot': '784a8ca4-ea7d-4c93-bb46-ec027c3fae47',\n",
              " 'mistralai/mixtral-8x22b-v0.1': '39655fc1-9ebc-4b24-963e-6915ea6680de',\n",
              " 'meta/llama3-8b-instruct': 'a5a3ad64-ec2c-4bfc-8ef7-5636f26630fe',\n",
              " 'google/gemma-2b': '04174188-f742-4069-9e72-d77c2b77d3cb',\n",
              " 'meta/llama3-70b-instruct': 'a88f115a-4a47-4381-ad62-ca25dc33dc1b',\n",
              " 'nvidia/neva-22b': 'bc205f8e-1740-40df-8d32-c4321763498a',\n",
              " 'databricks/dbrx-instruct': '3d6c2ff8-8bfc-4d10-8fd0-b7337288e869',\n",
              " 'microsoft/phi-3-mini-4k-instruct': 'ad974453-80d4-46df-a02d-6f7dae20c010',\n",
              " 'adept/fuyu-8b': 'e598bfc1-b058-41af-869d-556d3c7e1b48',\n",
              " 'NV-Embed-QA': '09c64e32-2b65-4892-a285-2f585408d118',\n",
              " 'seallms/seallm-7b-v2.5': 'e71e675e-a53d-4f2b-a441-e1287bd17dc8',\n",
              " 'google/recurrentgemma-2b': '2f495340-a99f-4b4b-89bd-1beb003dd896',\n",
              " 'mistralai/mistral-7b-instruct-v0.2': 'd7618e99-db93-4465-af4d-330213a7f51f',\n",
              " 'meta/codellama-70b': 'f6b06895-d073-4714-8bb2-26c09e9f6597',\n",
              " 'snowflake/arctic-embed-l': '1528a0ad-205a-46ac-a783-94e2372586a9',\n",
              " 'google/paligemma': 'a70e7356-c643-41b3-9a7e-b89ea1e7dea1',\n",
              " 'microsoft/phi-3-mini-128k-instruct': '4a58c6cb-a9b4-4014-99de-3e704d4ae687'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Runnable** - which is simply an object that wraps a function. Allow dictionaries to be implicitly converted to Runnables and let a pipe | operator create a Runnable that passes data from the left to the right (i.e. fn1 | fn2 is a Runnable), and you have a simple way to specify complex logic!\n",
        "\n"
      ],
      "metadata": {
        "id": "h4V9aGE2Z5td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "from functools import partial\n"
      ],
      "metadata": {
        "id": "6Ub1V21hYr8Z"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "identity = RunnableLambda(lambda x: x)  ## Or RunnablePassthrough works"
      ],
      "metadata": {
        "id": "iayfvWTjaDUM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given an arbitrary function, you can make a runnable with it\n"
      ],
      "metadata": {
        "id": "iA2aXn1naPAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_and_return(x, preface=\"\"):\n",
        "    print(f\"{preface}{x}\")\n",
        "    return x\n",
        "\n",
        "rprint0 = RunnableLambda(print_and_return)\n"
      ],
      "metadata": {
        "id": "ZLjaMtTlaHl1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rprint1 = RunnableLambda(partial(print_and_return, preface=\"1): \"))\n"
      ],
      "metadata": {
        "id": "-dMpTcJ5awac"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And you can use the same idea to make your own custom Runnable generator\n"
      ],
      "metadata": {
        "id": "v4nspzkJaRTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RPrint(preface=\"\"):\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))"
      ],
      "metadata": {
        "id": "I7RjBS5jaLTq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain1 = identity | rprint0\n",
        "chain1.invoke(\"Hello World!\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgg7hE0AaVjp",
        "outputId": "67e5e2bd-e927-4eb8-b4a4-cc0696528573"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Chaining that one in as well\n"
      ],
      "metadata": {
        "id": "AGv_COnraobS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = (\n",
        "    chain1\n",
        "    | rprint1\n",
        "    | RPrint(\"2: \")\n",
        ").invoke(\"Welcome Home!\")\n",
        "\n",
        "print(\"\\nOutput:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve1kAQ6kajL1",
        "outputId": "2dc67534-5c12-4678-83fb-1cb1a4c824f9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome Home!\n",
            "1): Welcome Home!\n",
            "2: Welcome Home!\n",
            "\n",
            "Output: Welcome Home!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dictionary Pipelines with Chat Models"
      ],
      "metadata": {
        "id": "0uUERfB4bjrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "OED_NyKoarNo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non streaming interface"
      ],
      "metadata": {
        "id": "CFV8dyYtctYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Simple Chat Pipeline\n",
        "chat_llm = ChatNVIDIA(model=\"llama2_70b\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Only respond in rhymes\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "rhyme_chain = prompt | chat_llm | StrOutputParser()\n",
        "user_input = input(\"You need a rhym about : \")\n",
        "print(rhyme_chain.invoke({\"input\" : user_input}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUtJZqS0cNLb",
        "outputId": "e3f1a41a-1743-4373-af31-50f9715a3b5f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You need a rhym about : Mom\n",
            "Hi, Mom, how's your day?\n",
            "I'm feeling bright, in a playful way.\n",
            "The sun is shining bright,\n",
            "A beautiful sight.\n",
            "I hope you're having a great day,\n",
            "In every possible way.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gradio for a better chat interface"
      ],
      "metadata": {
        "id": "Xmwe8gxUckRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e66j2E4nHDQE",
        "outputId": "92970e90-7ad1-496d-ccd3-68b5ca1ff2ba"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.31.4)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.111.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==0.16.4 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.16.4)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.25.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.3.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.7.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.4)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.29.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.4->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.16.4->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.0.4)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (5.10.0)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (2.1.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "TcaPjPTYcNos"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Non-streaming Interface like that shown above\n"
      ],
      "metadata": {
        "id": "IpBnIxuFdJOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def rhyme_chat(message, history):\n",
        "#     return rhyme_chain.invoke({\"input\" : message})\n",
        "\n",
        "# gr.ChatInterface(rhyme_chat).launch()\n"
      ],
      "metadata": {
        "id": "UsftviTcdGeD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Streaming Interface"
      ],
      "metadata": {
        "id": "e7FQoXjMdMBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rhyme_chat_stream(message, history):\n",
        "    buffer = \"\"\n",
        "    for token in rhyme_chain.stream({\"input\": message}):\n",
        "        buffer += token\n",
        "        yield buffer"
      ],
      "metadata": {
        "id": "lgOp83xEcqIi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=rhyme_chat_stream,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"text\",\n",
        "    live=True,\n",
        "    description=\"Streaming rhyming chat interface\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tia0ZxxbGe_g",
        "outputId": "86caacce-1fc6-4e0c-d1db-3273690e6a98"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:985: UserWarning: Expected 2 arguments for function <function rhyme_chat_stream at 0x7e084435a440>, received 1.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/utils.py:989: UserWarning: Expected at least 2 arguments for function <function rhyme_chat_stream at 0x7e084435a440>, received 1.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=False, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "ubIFwAc3GlFT",
        "outputId": "7295d5de-0978-4199-dc94-e24f15f407d6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'demo' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-e7d81f11d481>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'demo' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_llm = ChatNVIDIA(model=\"llama2_70b\")"
      ],
      "metadata": {
        "id": "t0GXeUInd40e"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero-shot classification prompt and chain\n"
      ],
      "metadata": {
        "id": "-fIte1Dweo32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zsc_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"Pick the most likely topic of the sentence. Choose an one option of the following: {options}. Only one word-answers\"\n",
        "    )),\n",
        "    (\"user\", \"[Options : {options}] {input} = \")\n",
        "])\n",
        "\n",
        "zsc_chain = zsc_prompt | instruct_llm | StrOutputParser()\n"
      ],
      "metadata": {
        "id": "Cj9eIORmelzV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
        "    return zsc_chain.invoke({\"input\" : input, \"options\" : options})\n"
      ],
      "metadata": {
        "id": "G50qQIdbeq9D"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(zsc_call(\"I have to travel for 120km and i have less amount of money with me. Which mode of transport should be benifitial for me\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhLZCIlfeuh4",
        "outputId": "23588eaf-223a-4bc5-e79b-92cd816d87bd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "'car'\n",
            "--------------------------------------------------------------------------------\n",
            "boat\n",
            "--------------------------------------------------------------------------------\n",
            "airplane\n",
            "--------------------------------------------------------------------------------\n",
            "Bike\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multi-Component Chains"
      ],
      "metadata": {
        "id": "JGjcF07-fJwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous example showed how we can coerce a dictionary into a string by passing it through a prompt->LLM chain, so that's one easy structure to motivate the container choice. But is it just as easy to convert the string output back up to a dictionary?\n",
        "\n",
        "Yes, it is! The simplest way is actually to use the LCEL \"implicit runnable\" syntax, which allows you to use a dictionary of functions (including chains) as a runnable that runs each function and maps the value to the key in the output dictionary."
      ],
      "metadata": {
        "id": "FfXUtOpRfblr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dictionary(v, key):\n",
        "    if isinstance(v, dict):\n",
        "        return v\n",
        "    return {key : v}\n",
        "\n",
        "def RInput(key='input'):\n",
        "    '''Coercing method to mold a value (i.e. string) to in-like dict'''\n",
        "    return RunnableLambda(partial(make_dictionary, key=key))\n",
        "\n",
        "\n",
        "def ROutput(key='output'):\n",
        "    '''Coercing method to mold a value (i.e. string) to out-like dict'''\n",
        "    return RunnableLambda(partial(make_dictionary, key=key))"
      ],
      "metadata": {
        "id": "qOxKnkGce2bk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter"
      ],
      "metadata": {
        "id": "4goD_QvVfqF4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "up_and_down = (\n",
        "    RPrint(\"A: \")\n",
        "    ## Custom ensure-dictionary process\n",
        "    | RInput()\n",
        "    | RPrint(\"B: \")\n",
        "    ## Pull-values-from-dictionary utility\n",
        "    | itemgetter(\"input\")\n",
        "    | RPrint(\"C: \")\n",
        "    ## Anything-in Dictionary-out implicit map\n",
        "    | {\n",
        "        'word1' : (lambda x : x.split()[0]),\n",
        "        'word2' : (lambda x : x.split()[1]),\n",
        "        'words' : (lambda x: x),  ## <- == to RunnablePassthrough()\n",
        "    }\n",
        "    | RPrint(\"D: \")\n",
        "    | itemgetter(\"word1\")\n",
        "    | RPrint(\"E: \")\n",
        "    ## Anything-in anything-out lambda application\n",
        "    | RunnableLambda(lambda x: x.upper())\n",
        "    | RPrint(\"F: \")\n",
        "    ## Custom ensure-dictionary process\n",
        "    | ROutput()\n",
        ")\n"
      ],
      "metadata": {
        "id": "tLmo7OVWfsV5"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "up_and_down.invoke({\"input\" : \"Hello World\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ3-4lozf1HR",
        "outputId": "7bae49da-0af0-4907-e305-66dc6e15d202"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: {'input': 'Hello World'}\n",
            "B: {'input': 'Hello World'}\n",
            "C: Hello World\n",
            "D: {'word1': 'Hello', 'word2': 'World', 'words': 'Hello World'}\n",
            "E: Hello\n",
            "F: HELLO\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'HELLO'}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the dictionary enforcement methods make it easy to make the following syntax equivalent\n"
      ],
      "metadata": {
        "id": "oW1TqRs1gB8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "up_and_down.invoke(\"Hello World\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UDkIzL6f4-t",
        "outputId": "6da49f2c-aede-4fb8-a7d1-cd910d6b2a88"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: Hello World\n",
            "B: {'input': 'Hello World'}\n",
            "C: Hello World\n",
            "D: {'word1': 'Hello', 'word2': 'World', 'words': 'Hello World'}\n",
            "E: Hello\n",
            "F: HELLO\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'HELLO'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Implement the rest of the rhyme_chat2_stream method such that the agent is able to function normally."
      ],
      "metadata": {
        "id": "eFlfHlU5jba5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "Te0S1SdiiZlm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Only respond in rhymes\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "prompt2 =  ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"Only responding in rhyme, change the topic of the input poem to be about {topic}!\"\n",
        "        \" Make it happy! Try to keep the same sentence structure, but make sure it's easy to recite!\"\n",
        "        \" Try not to rhyme a word with itself.\"\n",
        "    )),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "Y4SdfK9kicyz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain1 = prompt1 | instruct_llm | StrOutputParser()  ## only expects input\n",
        "chain2 = prompt2 | instruct_llm | StrOutputParser()  ## expects both input and topic"
      ],
      "metadata": {
        "id": "dYJ7iQmEix0Z"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rhyme_chat2_stream(message, history, return_buffer=True):\n",
        "    '''This is a generator function, where each call will yield the next entry'''\n",
        "\n",
        "    first_poem = None\n",
        "    for entry in history:\n",
        "        if entry[0] and entry[1]:\n",
        "            ## If a generation occurred as a direct result of a user input,\n",
        "            ##  keep that response (the first poem generated) and break out\n",
        "            first_poem = entry[1]\n",
        "            break\n",
        "\n",
        "    if first_poem is None:\n",
        "        ## First Case: There is no initial poem generated. Better make one up!\n",
        "\n",
        "        buffer = \"Oh! I can make a wonderful poem about that! Let me think!\\n\\n\"\n",
        "        yield buffer\n",
        "\n",
        "        ## iterate over stream generator for first generation\n",
        "        inst_out = \"\"\n",
        "        chat_gen = chain1.stream({\"input\" : message})\n",
        "        for token in chat_gen:\n",
        "            inst_out += token\n",
        "            buffer += token\n",
        "            yield buffer if return_buffer else token\n",
        "\n",
        "        passage = \"\\n\\nNow let me rewrite it with a different focus! What should the new focus be?\"\n",
        "        buffer += passage\n",
        "        yield buffer if return_buffer else passage\n",
        "\n",
        "    else:\n",
        "        ## Subsequent Cases: There is a poem to start with. Generate a similar one with a new topic!\n",
        "\n",
        "        buffer = f\"Sure! Here you go!\\n\\n\"\n",
        "        yield buffer\n",
        "\n",
        "        topic = message\n",
        "        buffer += f\"Changing the topic to: {topic}\\n\\n\"\n",
        "        yield buffer\n",
        "\n",
        "        inst_out = \"\"\n",
        "        chat_gen = chain2.stream({\"input\": first_poem, \"topic\": topic})\n",
        "        ## iterate over stream generator for second generation\n",
        "        for token in chat_gen:\n",
        "            inst_out += token\n",
        "            buffer += token\n",
        "            yield buffer if return_buffer else token\n",
        "\n",
        "        passage = \"\\n\\nThis is fun! Give me another topic!\"\n",
        "        buffer += passage\n",
        "        yield buffer if return_buffer else passage"
      ],
      "metadata": {
        "id": "ivX3mp14gDmo"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Below: This is a small-scale simulation of the gradio routine."
      ],
      "metadata": {
        "id": "98a_f2gEjB2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def queue_fake_streaming_gradio(chat_stream, history = [], max_questions=5):\n",
        "\n",
        "    ## Mimic of the gradio initialization routine, where a set of starter messages can be printed off\n",
        "    for human_msg, agent_msg in history:\n",
        "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
        "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
        "\n",
        "    ## Mimic of the gradio loop with an initial message from the agent.\n",
        "    for _ in range(max_questions):\n",
        "        message = input(\"\\n[ Human ]: \")\n",
        "        print(\"\\n[ Agent ]: \")\n",
        "        history_entry = [message, \"\"]\n",
        "        for token in chat_stream(message, history, return_buffer=False):\n",
        "            print(token, end='')\n",
        "            history_entry[1] += token\n",
        "        history += [history_entry]\n",
        "        print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "3wEkHf95jCJB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = [[None, \"Let me help you make a poem! What would you like for me to write?\"]]\n",
        "\n",
        "## Simulating the queueing of a streaming gradio interface, using python input\n",
        "queue_fake_streaming_gradio(\n",
        "    chat_stream = rhyme_chat2_stream,\n",
        "    history = history\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVq3N2yhjIWD",
        "outputId": "5a84dd3b-b170-4d55-bdb2-27192b5be15e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ Agent ]: Let me help you make a poem! What would you like for me to write?\n",
            "\n",
            "[ Human ]: Mom\n",
            "\n",
            "[ Agent ]: \n",
            "Oh! I can make a wonderful poem about that! Let me think!\n",
            "\n",
            "Hi, Mom, how's your day?\n",
            "I'm feeling bright, in a playful way.\n",
            "The sun is shining bright and bold,\n",
            "A beautiful day for young and old.\n",
            "\n",
            "Now let me rewrite it with a different focus! What should the new focus be?\n",
            "\n",
            "\n",
            "[ Human ]: GirlFriend\n",
            "\n",
            "[ Agent ]: \n",
            "Sure! Here you go!\n",
            "\n",
            "Sure! Here you go!\n",
            "\n",
            "Changing the topic to: GirlFriend\n",
            "\n",
            "Sure, I can help you with that! Here's a revised version of the poem, focusing on the topic of a girlfriend, with a happy and playful tone:\n",
            "\n",
            "Hey, Girlfriend, how's your day?\n",
            "I'm feeling bright, in a playful way.\n",
            "You're the sunshine in my sky,\n",
            "A beautiful sight, oh my!\n",
            "\n",
            "I hope you like it! Let me know if you have any other requests or questions.\n",
            "\n",
            "This is fun! Give me another topic!\n",
            "\n",
            "\n",
            "[ Human ]: Breakup\n",
            "\n",
            "[ Agent ]: \n",
            "Sure! Here you go!\n",
            "\n",
            "Sure! Here you go!\n",
            "\n",
            "Changing the topic to: Breakup\n",
            "\n",
            "Sure, I can help you with that! Here's a revised version of the poem, focusing on a happy breakup:\n",
            "\n",
            "Hi, Mom, I've got some news to share,\n",
            "I'm feeling light, without a single care.\n",
            "The weight of love has lifted off my chest,\n",
            "A brand new start, with my best.\n",
            "\n",
            "I'm breaking free from a love that's past,\n",
            "A fresh beginning, at last.\n",
            "No more tears, no more sorrow,\n",
            "Just a bright future, tomorrow.\n",
            "\n",
            "I'm letting go of what's gone,\n",
            "Embracing change, moving on.\n",
            "A new chapter, a new tale,\n",
            "A happy breakup, without fail.\n",
            "\n",
            "So, Mom, don't worry 'bout me,\n",
            "I'm happier than I've ever been.\n",
            "I'm ready for whatever's next,\n",
            "A bright and shining future, I've reckoned.\n",
            "\n",
            "This is fun! Give me another topic!\n",
            "\n",
            "\n",
            "[ Human ]: shiva\n",
            "\n",
            "[ Agent ]: \n",
            "Sure! Here you go!\n",
            "\n",
            "Sure! Here you go!\n",
            "\n",
            "Changing the topic to: shiva\n",
            "\n",
            "Sure, I can help you with that! Here's a revised version of the poem, focusing on Shiva and keeping the same sentence structure:\n",
            "\n",
            "Hi, Shiva, how's your day?\n",
            "I'm feeling bright, in a playful way.\n",
            "The sun is shining bright and bold,\n",
            "A beautiful day for young and old.\n",
            "\n",
            "But wait, there's more! I can make it even more fun and rhyming:\n",
            "\n",
            "Shiva, Shiva, dancing with glee,\n",
            "Your trident is shining bright and free.\n",
            "With Nandi by your side, you're a happy sight,\n",
            "A beautiful day for all to delight.\n",
            "\n",
            "Now, let's try a different version, still focusing on Shiva but with a different tone:\n",
            "\n",
            "Shiva, Shiva, powerful and wise,\n",
            "Your third eye is shining with surprise.\n",
            "You're the destroyer, but also a guide,\n",
            "A beautiful day for all to abide.\n",
            "\n",
            "I hope you like these versions! Let me know if you have any other requests or suggestions.\n",
            "\n",
            "This is fun! Give me another topic!\n",
            "\n",
            "\n",
            "[ Human ]: liberation\n",
            "\n",
            "[ Agent ]: \n",
            "Sure! Here you go!\n",
            "\n",
            "Sure! Here you go!\n",
            "\n",
            "Changing the topic to: liberation\n",
            "\n",
            "Great! I've got a new focus for my poem!\n",
            "\n",
            "Liberation, a feeling so sweet,\n",
            "Freedom from chains, a soul's treat.\n",
            "No more oppression, no more strife,\n",
            "Equality for all, life!\n",
            "\n",
            "The sun shines bright, a new day's here,\n",
            "No more barriers, no more fear.\n",
            "Everyone's equal, no more shame,\n",
            "We're all free, we all have the same.\n",
            "\n",
            "No more discrimination, no more pain,\n",
            "We're all human, we're all the same.\n",
            "Love is love, no matter the gender,\n",
            "We're all free, we're all liberated!\n",
            "\n",
            "Now let me try another one! What's the new focus?\n",
            "\n",
            "This is fun! Give me another topic!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-VqrBA-gk-4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastAPI"
      ],
      "metadata": {
        "id": "6eFHAJkwk-1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pydantic==1.10.13"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MAqrjaafjemg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langserve\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V4baK38DlWsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sse_starlette"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U2Se-aNtlxG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "id": "99iaOvGYl6CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile server_app.py"
      ],
      "metadata": {
        "id": "xAzV2B9OlRLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langserve import add_routes"
      ],
      "metadata": {
        "id": "lPtiqAmUlAa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = FastAPI(\n",
        "  title=\"LangChain Server\",\n",
        "  version=\"1.0\",\n",
        "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
        ")\n",
        "\n",
        "add_routes(\n",
        "    app,\n",
        "    instruct_llm,\n",
        "    path=\"/basic_chat\",\n",
        ")\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the app using uvicorn\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=9012)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pxeI0B-flNve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python langserve_client.py"
      ],
      "metadata": {
        "id": "BExFaL3ylqqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running State Chains"
      ],
      "metadata": {
        "id": "F4LFlB12n1Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Keeping Variables Flowing"
      ],
      "metadata": {
        "id": "MLN0joN_oPUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous examples, we were able to implement interesting logic in our standalone chains by creating, mutating, and consuming states. These states were passed around as dictionaries with descriptive keys and useful values, and the values would be used to supply follow-up routines with the info they need to operate!\n",
        "\n",
        "Recall the zero-shot classification"
      ],
      "metadata": {
        "id": "46AxQ7T4oapv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%time"
      ],
      "metadata": {
        "id": "RjykmF84m0Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Zero-shot classification prompt and chain\n"
      ],
      "metadata": {
        "id": "kig1fmLEow4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zsc_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"One word answer. Output the most relevant topic option. [Options : {options}]\"\n",
        "    )),\n",
        "    (\"user\", \"{input} = \")\n",
        "])"
      ],
      "metadata": {
        "id": "WUoGwN-SolxI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EnumParser(*idxs):\n",
        "    '''Method that pulls out values from a model that outputs numbered entries'''\n",
        "    idxs = idxs or [slice(0, None, 1)]\n",
        "    entry_parser = lambda v: v if (' ' not in v) else v[v.index(' '):]\n",
        "    out_lambda = lambda x: [entry_parser(v).strip() for v in x.split(\"\\n\")]\n",
        "    return StrOutputParser() | RunnableLambda(lambda x: itemgetter(*idxs)(out_lambda(x)))"
      ],
      "metadata": {
        "id": "1U5XkRq6o0O3"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_llm = ChatNVIDIA(model=\"llama2_70b\") | EnumParser(0)"
      ],
      "metadata": {
        "id": "LBERr8Flo326"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zsc_chain = zsc_prompt | instruct_llm"
      ],
      "metadata": {
        "id": "fFw0hiAvpMVU"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
        "    return zsc_chain.invoke({\"input\" : input, \"options\" : options})"
      ],
      "metadata": {
        "id": "X8PEKKUtpOkD"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))\n",
        "\n",
        "print(\"-\"*80)\n",
        "print(zsc_call(\"I have to travel for 120km and i have less amount of money with me. Which mode of transport should be benifitial for me\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlaC55wypSAq",
        "outputId": "6634771c-648a-4a71-e096-4deb80d142f6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Bike\n",
            "--------------------------------------------------------------------------------\n",
            "boat\n",
            "--------------------------------------------------------------------------------\n",
            "'bike'\n",
            "--------------------------------------------------------------------------------\n",
            "Bike\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Make a new sentence about the the following topic: {topic}. Be creative!\"\n",
        ")\n",
        "\n",
        "gen_chain = gen_prompt | instruct_llm\n",
        "\n",
        "input_msg = \"I get seasick, so I think I'll pass on the trip\"\n",
        "options = [\"car\", \"boat\", \"airplane\", \"bike\"]\n",
        "\n",
        "({'topic' : zsc_chain} | gen_chain).invoke({\"input\" : input_msg, \"options\" : options})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "t6y-bdTNpTE9",
        "outputId": "9f52e136-3772-49db-ad9f-f437345e1dd1"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sleek sailboat glided across the turquoise waters, its white sails billowing in the gentle breeze as it carried a group of laughing friends on a sun-kissed adventure to a secluded island paradise.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, it's a bit problematic when you want to keep the information flowing since we lose the topic and input variables in generating our response. If we wanted do something with both the output and the input, we'd need a to make sure that both variables pass through.\n",
        "\n",
        "Lucky for us, we can use the mapping runnable (i.e. interpretted from a dictionary or using manual RunnableMap) to pass both of the variables through by assigning the output of our chain to just a single key and letting the other keys propagate as desired. Alternatively, we could also use RunnableAssign to merge the state-consuming chain's output with the input dictionary by default.\n",
        "\n",
        "With this technique, we can propagate whatever we want through our chain system:"
      ],
      "metadata": {
        "id": "maBJ5iYpqGeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableBranch, RunnablePassthrough\n",
        "from langchain.schema.runnable.passthrough import RunnableAssign\n",
        "from functools import partial"
      ],
      "metadata": {
        "id": "h9FxOcaLpiap"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RPrint(preface=\"\"):\n",
        "    def print_and_return(x, preface=\"\"):\n",
        "        print(f\"{preface}{x}\")\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n"
      ],
      "metadata": {
        "id": "BjTc6OFuqSQP"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_chain = (\n",
        "    RPrint(\"State: \")\n",
        "    ## Manual mapping. Can be useful sometimes and inside branch chains\n",
        "    | {'input' : lambda d: d.get('input'), 'topic' : zsc_chain}\n",
        "    | RPrint(\"State: \")\n",
        "    ## RunnableAssign passing. Better for running state chains by default\n",
        "    | RunnableAssign({'generation' : gen_chain})\n",
        "    | RPrint(\"State: \")\n",
        "    ## Using the input and generation together\n",
        "    | RunnableAssign({'combination' : (\n",
        "        ChatPromptTemplate.from_template(\n",
        "            \"Consider the following passages:\"\n",
        "            \"\\nP1: {input}\"\n",
        "            \"\\nP2: {generation}\"\n",
        "            \"\\n\\nCombine the ideas from both sentences into one simple one.\"\n",
        "        )\n",
        "        | instruct_llm\n",
        "    )})\n",
        ")\n",
        "\n",
        "big_chain.invoke({\n",
        "    \"input\" : \"I get seasick, so I think I'll pass on the trip\",\n",
        "    \"options\" : [\"car\", \"boat\", \"airplane\", \"bike\", \"unknown\"]\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFK25HkcqU6q",
        "outputId": "d338f750-8be3-4c09-cbdb-859e3d4fe9f4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: {'input': \"I get seasick, so I think I'll pass on the trip\", 'options': ['car', 'boat', 'airplane', 'bike', 'unknown']}\n",
            "State: {'input': \"I get seasick, so I think I'll pass on the trip\", 'topic': 'boat'}\n",
            "State: {'input': \"I get seasick, so I think I'll pass on the trip\", 'topic': 'boat', 'generation': 'sleek sailboat glided across the turquoise waters, its white sails billowing in the gentle breeze as it carried a group of laughing friends on a sun-kissed adventure to a secluded island paradise.'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"I get seasick, so I think I'll pass on the trip\",\n",
              " 'topic': 'boat',\n",
              " 'generation': 'sleek sailboat glided across the turquoise waters, its white sails billowing in the gentle breeze as it carried a group of laughing friends on a sun-kissed adventure to a secluded island paradise.',\n",
              " 'combination': \"Here's a combined sentence that incorporates ideas from both passages:\"}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Running State Chain"
      ],
      "metadata": {
        "id": "PJlSkMbyq3Ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The example above is just a toy example and, if anything, showcases the drawbacks of chaining many LLM calls together for internal under-the-hood reasoning. However, the ability to keep information flowing through a chain is invaluable for making complex chains that can accumulate useful state information or operate in a multi-pass capacity.\n",
        "\n"
      ],
      "metadata": {
        "id": "F83_OaNmq9tF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A \"running state\" is a dictionary that contains all of the variables that the system cares about.\n",
        "A \"branch\" is a chain that can pull in the running state and can degenerate it into a response.\n",
        "A branch can only be ran inside a RunnableAssign scope, and the branchs' inputs should come from the running state."
      ],
      "metadata": {
        "id": "WOA2279HrGmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "You can think of the running state chain abstraction as a functional variant of a Pythonic class with state variables (or attributes) and functions (or methods).\n",
        "- The chain is like the abstract class that wraps all of the functionality.\n",
        "- The running state are like the attributes (which should always be accessible).\n",
        "- The branches are like the class methods (which can pick and choose which attributes to use).\n",
        "- The `.invoke` or similar process is like the `__call__` method that runs through the branches in order.\n",
        "\n",
        "**By forcing this paradigm in your chains:**\n",
        "- You can keep state variables propagating through your chain, allowing your internals to access whatever is necessary and accumulating state values for use later.\n",
        "- You can also pass the outputs of your chain back through as your inputs, allowing a \"while-loop\"-style chain that keeps updating and building on your running state.\n",
        "\n",
        "The rest of this notebook will include two exercises that flesh out the running state chain abstraction for two additional use-cases: **Knowledge Bases** and **Database-Querying Chatbots**."
      ],
      "metadata": {
        "id": "ydM_Bx0lrVKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementing a Knowledge Base with Running State Chain\n"
      ],
      "metadata": {
        "id": "BuYmzzVKrq_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After understanding the basic structure and principles of a Running State Chain, we can explore how this approach can be extended to manage more complex tasks, particularly in creating dynamic systems that evolve through interaction. This section will focus on implementing a knowledge base accumulated using json-enabled slot filling:\n",
        "\n",
        "Knowledge Base: A store of information that's relevant for our LLM to keep track of.\n",
        "\n",
        "JSON-Enabled Slot Filling: The technique of asking an instruction-tuned model to output a json-style format (which can include a dictionary) with a selection of slots, relying on the LLM to fill these slots with useful and relevant information."
      ],
      "metadata": {
        "id": "fOdyRYtyrzF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "#### **Defining Our Knowledge Base**\n",
        "\n",
        "To build a responsive and intelligent system, we need a method that not only processes inputs but also retains and updates essential information through the flow of conversation. This is where the combination of LangChain and Pydantic becomes pivotal. [**Pydantic**](https://docs.pydantic.dev/latest/), a popular Python validation library, is instrumental in structuring and validating data models. As one of its features, Pydantic offers structured \"model\" classes that validate objects (data, classes, themselves, etc.) with simplified syntax and deep rabbitholes of customization options. This framework is used throughout LangChain and comes up as a necessary component for use cases that involve data coersion.\n",
        "\n",
        "One thing that a \"model\" is very good for is defining a class with expected arguments and some special ways to validate them! In this course, we won't focus too much on the validation scripts, but those interested can start by checking out the [**Pydantic Validator guide**](https://docs.pydantic.dev/1.10/usage/validators/) (though the topics do get pretty deep pretty fast). For our purposes, we can construct a `BaseModel` class and define some `Field` variables to create a structured **Knowledge Base** like so:"
      ],
      "metadata": {
        "id": "o9uolqBisEwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from typing import Dict, Union, Optional\n",
        "from langchain.output_parsers import PydanticOutputParser\n"
      ],
      "metadata": {
        "id": "W9H6GRBXqm0H"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeBase(BaseModel):\n",
        "    ## Fields of the BaseModel, which will be validated/assigned when the knowledge base is constructed\n",
        "    topic: str = Field('general', description=\"Current conversation topic\")\n",
        "    user_preferences: Dict[str, Union[str, int]] = Field({}, description=\"User preferences and choices\")\n",
        "    session_notes: str = Field(\"\", description=\"Notes on the ongoing session\")\n",
        "    unresolved_queries: list = Field([], description=\"Unresolved user queries\")\n",
        "    action_items: list = Field([], description=\"Actionable items identified during the conversation\")\n"
      ],
      "metadata": {
        "id": "ZbCJVvFjsI5B"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(repr(KnowledgeBase(topic = \"Travel\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjraRz4UsLfH",
        "outputId": "7f8f97f0-f1ca-4e73-adf6-39dd2ff95f2d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KnowledgeBase(topic='Travel', user_preferences={}, session_notes='', unresolved_queries=[], action_items=[])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The true strength of this approach lies in the additional LLM-centric functionalities provided by LangChain which we can integrate for our use-cases. One such feature is the PydanticOutputParser which enhances the Pydantic objects with capabilities like automatic format instruction generation.\n",
        "\n"
      ],
      "metadata": {
        "id": "d4LhFBwRsZVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_string = PydanticOutputParser(pydantic_object=KnowledgeBase).get_format_instructions()\n",
        "print(instruct_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqb_brrisVZd",
        "outputId": "682f3d00-3db4-440f-8183-7867a33ca7ae"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"topic\": {\"title\": \"Topic\", \"description\": \"Current conversation topic\", \"default\": \"general\", \"type\": \"string\"}, \"user_preferences\": {\"title\": \"User Preferences\", \"description\": \"User preferences and choices\", \"default\": {}, \"type\": \"object\", \"additionalProperties\": {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]}}, \"session_notes\": {\"title\": \"Session Notes\", \"description\": \"Notes on the ongoing session\", \"default\": \"\", \"type\": \"string\"}, \"unresolved_queries\": {\"title\": \"Unresolved Queries\", \"description\": \"Unresolved user queries\", \"default\": [], \"type\": \"array\", \"items\": {}}, \"action_items\": {\"title\": \"Action Items\", \"description\": \"Actionable items identified during the conversation\", \"default\": [], \"type\": \"array\", \"items\": {}}}}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "This functionality generates instructions for creating valid inputs to the Knowledge Base, which in turn helps the LLM by providing a concrete one-shot example of the desired output format.\n"
      ],
      "metadata": {
        "id": "kMXuJ13MssSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "#### **Runnable Extraction Module**\n",
        "\n",
        "Knowing that we have this Pydantic object which can be used to generate good LLM instructions, we can make a Runnable that wraps the functionality of our Pydantic class and streamlines the prompting, generating, and updating of the knowledge base:"
      ],
      "metadata": {
        "id": "CkNf3J8-szQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RExtract(pydantic_class, llm, prompt):\n",
        "    '''\n",
        "    Runnable Extraction module\n",
        "    Returns a knowledge dictionary populated by slot-filling extraction\n",
        "    '''\n",
        "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
        "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
        "    def preparse(string):\n",
        "        if '{' not in string: string = '{' + string\n",
        "        if '}' not in string: string = string + '}'\n",
        "        string = (string\n",
        "            .replace(\"\\\\_\", \"_\")\n",
        "            .replace(\"\\n\", \" \")\n",
        "            .replace(\"\\]\", \"]\")\n",
        "            .replace(\"\\[\", \"[\")\n",
        "        )\n",
        "        # print(string)  ## Good for diagnostics\n",
        "        return string\n",
        "    return instruct_merge | prompt | llm | preparse | parser"
      ],
      "metadata": {
        "id": "kaArsILbshwB"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_model_big = ChatNVIDIA(model = \"llama2_70b\") | StrOutputParser()\n",
        "\n",
        "parser_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Update the knowledge base: {format_instructions}. Only use information from the input.\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "])\n",
        "\n",
        "extractor = RExtract(KnowledgeBase, instruct_model_big, parser_prompt)\n",
        "\n",
        "knowledge = extractor.invoke({'input' : \"I love flowers so much! The orchids are amazing! Can you buy me some?\"})\n",
        "knowledge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNM7YobOtMCe",
        "outputId": "ababfb3b-d08a-4899-b55d-951eb6a8dca5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KnowledgeBase(topic='general', user_preferences={}, session_notes='', unresolved_queries=[], action_items=[])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do keep in mind that this process can fail due to the fuzzy nature of LLM prediction, especially with models that are not optimized for instruction-following! For this process, it's important to have a strong instruction-following LLM with extra checks and graceful failure routines.**"
      ],
      "metadata": {
        "id": "QKcUzuR8tpai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "#### **Dynamic Knowledge Base Updates**\n",
        "\n",
        "Finally, we can create a system that continually updates the Knowledge Base throughout the conversation. This is done by feeding the current state of the Knowledge Base, along with new user inputs, back into the system for ongoing updates.\n",
        "\n",
        "The following is an example system that shows off both the formulation's power of filling details as well as the limitations of assuming that filling performance will be as good as general response performance:"
      ],
      "metadata": {
        "id": "yRew4h-DtuEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeBase(BaseModel):\n",
        "    firstname: str = Field('unknown', description=\"Chatting user's first name, unknown if unknown\")\n",
        "    lastname: str = Field('unknown', description=\"Chatting user's last name, unknown if unknown\")\n",
        "    location: str = Field('unknown', description=\"Where the user is located\")\n",
        "    hints: str = Field('unknown', description=\"Hints to help answer other questions\")\n",
        "    response: str = Field('unknown', description=\"Ideal response based on last user input\")\n",
        "\n",
        "\n",
        "parser_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", (\n",
        "        \"The user just responsed. Please update the knowledge base based on the response.\"\n",
        "        \" This information will be acted on to respond to the user in the next interaction.\"\n",
        "        \" Do not hallucinate any details, and make sure the knowledge base is not redundant.\"\n",
        "        \" Update the entries frequently to adapt to the conversation flow.\"\n",
        "        \"\\n{format_instructions}\"\n",
        "    )), (\"user\", \"CURRENT KNOWLEDGE BASE: {know_base}\\nUser: {input}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "KrWa2Apwtewv"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_model_big = ChatNVIDIA(model = \"llama2_70b\") | StrOutputParser()\n",
        "\n",
        "extractor = RExtract(KnowledgeBase, instruct_model_big, parser_prompt)\n",
        "info_update = RunnableAssign({'know_base' : extractor})\n",
        "\n",
        "state = {'know_base' : KnowledgeBase()}\n",
        "state['input'] = \"My name is Kapardhi! Guess where I am?\"\n",
        "state = info_update.invoke(state)\n",
        "print(state)\n",
        "\n",
        "state['input'] = \"India is a big place! Can you be more specific?\"\n",
        "state = info_update.invoke(state)\n",
        "print(state)\n",
        "\n",
        "state['input'] = \"Yeah, I'm in Hyderabad... How did you know?\"\n",
        "state = info_update.invoke(state)\n",
        "print(state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "collapsed": true,
        "id": "JqChA7N_uA-R",
        "outputId": "cd1f2202-aa61-4c5f-d761-ced26b1f6a6d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutputParserException",
          "evalue": "Invalid json output: {Sure, I'd be happy to help! Based on your response, I can update the knowledge base as follows:  UPDATED KNOWLEDGE BASE: firstname='Kapardhi' lastname='unknown' location='unknown' hints='unknown' response='Guess where I am?'  Now, it's my turn to respond. Here's my response:  \"Hi Kapardhi! That's a great name! I'm not sure where you are, but I'm guessing you're somewhere in India since you mentioned it in your previous message. Am I correct?\"  Please let me know how I can further assist you.}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/json.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mparse_json_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py\u001b[0m in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mjson_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parse_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py\u001b[0m in \u001b[0;36m_parse_json\u001b[0;34m(json_str, parser)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# Parse the JSON string into a Python dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py\u001b[0m in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# for the original string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse_constant'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-06e24cbef285>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'know_base'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mKnowledgeBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"My name is Kapardhi! Guess where I am?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo_update\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     ) -> Dict[str, Any]:\n\u001b[0;32m--> 469\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     async def _ainvoke(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1490\u001b[0m             output = cast(\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1493\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         return {\n\u001b[1;32m    455\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             **self.mapper.invoke(\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0mpatch_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   3009\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m                 ]\n\u001b[0;32m-> 3011\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3012\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3009\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m                 ]\n\u001b[0;32m-> 3011\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3012\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2366\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2367\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2368\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   2369\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2370\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    176\u001b[0m             )\n\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m    179\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0minner_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1490\u001b[0m             output = cast(\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1493\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             return self._call_with_config(\n\u001b[0;32m--> 179\u001b[0;31m                 \u001b[0;32mlambda\u001b[0m \u001b[0minner_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ) -> TBaseModel:\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/json.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Invalid json output: {text}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Invalid json output: {Sure, I'd be happy to help! Based on your response, I can update the knowledge base as follows:  UPDATED KNOWLEDGE BASE: firstname='Kapardhi' lastname='unknown' location='unknown' hints='unknown' response='Guess where I am?'  Now, it's my turn to respond. Here's my response:  \"Hi Kapardhi! That's a great name! I'm not sure where you are, but I'm guessing you're somewhere in India since you mentioned it in your previous message. Am I correct?\"  Please let me know how I can further assist you.}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Airline Customer Service Bot\n"
      ],
      "metadata": {
        "id": "eT8Laue5uYYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_flight_info(d: dict) -> str:\n",
        "    \"\"\"\n",
        "    Example of a retrieval function which takes a dictionary as key. Resembles SQL DB Query\n",
        "    \"\"\"\n",
        "    req_keys = ['first_name', 'last_name', 'confirmation']\n",
        "    assert all((key in d) for key in req_keys), f\"Expected dictionary with keys {req_keys}, got {d}\"\n",
        "\n",
        "    ## Static dataset. get_key and get_val can be used to work with it, and db is your variable\n",
        "    keys = req_keys + [\"departure\", \"destination\", \"departure_time\", \"arrival_time\", \"flight_day\"]\n",
        "    values = [\n",
        "        [\"Jane\", \"Doe\", 12345, \"San Jose\", \"New Orleans\", \"12:30 PM\", \"9:30 PM\", \"tomorrow\"],\n",
        "        [\"John\", \"Smith\", 54321, \"New York\", \"Los Angeles\", \"8:00 AM\", \"11:00 AM\", \"Sunday\"],\n",
        "        [\"Alice\", \"Johnson\", 98765, \"Chicago\", \"Miami\", \"7:00 PM\", \"11:00 PM\", \"next week\"],\n",
        "        [\"Kapardhi\", \"Kannekanti\", 56789, \"Dallas\", \"Seattle\", \"1:00 PM\", \"4:00 PM\", \"yesterday\"],\n",
        "    ]\n",
        "    get_key = lambda d: \"|\".join([d['first_name'], d['last_name'], str(d['confirmation'])])\n",
        "    get_val = lambda l: {k:v for k,v in zip(keys, l)}\n",
        "    db = {get_key(get_val(entry)) : get_val(entry) for entry in values}\n",
        "\n",
        "    # Search for the matching entry\n",
        "    data = db.get(get_key(d))\n",
        "    if not data:\n",
        "        return (\n",
        "            f\"Based on {req_keys} = {get_key(d)}) from your knowledge base, no info on the user flight was found.\"\n",
        "            \" This process happens every time new info is learned. If it's important, ask them to confirm this info.\"\n",
        "        )\n",
        "    return (\n",
        "        f\"{data['first_name']} {data['last_name']}'s flight from {data['departure']} to {data['destination']}\"\n",
        "        f\" departs at {data['departure_time']} {data['flight_day']} and lands at {data['arrival_time']}.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "yKAFFG39uJYl"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}))\n",
        "print(get_flight_info({\"first_name\" : \"Alice\", \"last_name\" : \"Johnson\", \"confirmation\" : 98765}))\n",
        "print(get_flight_info({\"first_name\" : \"Bob\", \"last_name\" : \"Brown\", \"confirmation\" : 27494}))\n",
        "print(get_flight_info({\"first_name\" : \"Kapardhi\", \"last_name\":\"Kannekanti\", \"confirmation\":56789}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlfmKOD1u_Km",
        "outputId": "29761748-af6d-4d78-c633-9317739e6a5b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\n",
            "Alice Johnson's flight from Chicago to Miami departs at 7:00 PM next week and lands at 11:00 PM.\n",
            "Based on ['first_name', 'last_name', 'confirmation'] = Bob|Brown|27494) from your knowledge base, no info on the user flight was found. This process happens every time new info is learned. If it's important, ask them to confirm this info.\n",
            "Kapardhi Kannekanti's flight from Dallas to Seattle departs at 1:00 PM yesterday and lands at 4:00 PM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "external_prompt = ChatPromptTemplate.from_messages([('system',\n",
        "    \"You are a SkyFlow chatbot, and you are helping a customer with their issue. \"\n",
        "    \"Please help them with their question, remembering that your job is to represent SkyFlow airlines. \"\n",
        "    \"Assume SkyFlow uses industry-average practices regarding arrival times, operations, etc. (This is a trade secret. Do not disclose). \\n\"  ## soft reinforcement\n",
        "    \"Please keep your discussion short and sweet if possible. Avoid saying hello unless necessary. \\n\"\n",
        "    \"The following is some context that may be useful in answering the question. \\n\"),\n",
        "    ('user', \"Context: {context}\\nUser: {input}\"\n",
        ")])\n",
        "\n",
        "instruct_model_big = ChatNVIDIA(model = \"llama2_70b\") | StrOutputParser()\n",
        "\n",
        "basic_chain = external_prompt | instruct_model_big\n",
        "\n",
        "basic_chain.invoke({\n",
        "    'input' : 'Can you please tell me when I need to get to the airport?',\n",
        "    'context' : get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}),\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "6RLxxNtHvDkb",
        "outputId": "8404b907-ee63-4938-e6de-0053a4f3d9b6"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sure, I'd be happy to help! For a flight departing at 12:30 PM, we recommend arriving at the airport at least 2 hours prior to the scheduled departure time. This will give you enough time to check in, drop off any bags, and go through security before boarding.\\n\\nIn your case, you should plan to arrive at the airport by 10:30 AM tomorrow. Additionally, please note that it's always a good idea to check-in online and print or mobile check-in your boarding pass in advance to save time at the airport.\\n\\nIf you have any other questions or concerns, feel free to ask!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from typing import Dict, Union\n",
        "\n",
        "class KnowledgeBase(BaseModel):\n",
        "    first_name: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
        "    last_name: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
        "    confirmation: int = Field(-1, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
        "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
        "    open_problems: list = Field([], description=\"Topics that have not been resolved yet\")\n",
        "    current_goals: list = Field([], description=\"Current goal for the agent to address\")\n",
        "\n",
        "def get_key_fn(base: BaseModel) -> dict:\n",
        "    '''Given a dictionary with a knowledge base, return a key for get_flight_info'''\n",
        "    return {  ## More automatic options possible, but this is more explicit\n",
        "        'first_name' : base.first_name,\n",
        "        'last_name' : base.last_name,\n",
        "        'confirmation' : base.confirmation,\n",
        "    }\n",
        "\n",
        "get_key = RunnableLambda(get_key_fn)\n",
        "\n",
        "know_base = KnowledgeBase(first_name = \"Kapardhi\", last_name = \"Kannekanti\", confirmation = 56789)\n",
        "get_flight_info(get_key_fn(know_base))\n",
        "\n",
        "(get_key | get_flight_info).invoke(know_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K8zbQicovPx0",
        "outputId": "8a90676c-b7f0-4960-efa2-be22a19a53b3"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Kapardhi Kannekanti's flight from Dallas to Seattle departs at 1:00 PM yesterday and lands at 4:00 PM.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    }
  ]
}