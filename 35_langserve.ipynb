{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "YRX1R3GupzkZ",
   "metadata": {
    "id": "YRX1R3GupzkZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Part 1:** Delivering the /basic_chat endpoint\n",
    "\n",
    "Instructions are provided for launching a `/basic_chat` endpoint both as a standalone Python file. This will be used by the frontend to make basic decision with no internal reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "TniVLtL-qcqo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1702915515784,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "TniVLtL-qcqo",
    "outputId": "7ff6eb58-b9c1-4ce9-b15a-b1a515045ae0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting server_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile server_app.py\n",
    "# https://python.langchain.com/docs/langserve#server\n",
    "import typing\n",
    "import os\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "from fastapi import FastAPI\n",
    "from time import sleep\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.document_loaders import ArxivLoader\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "from langserve import RemoteRunnable, add_routes\n",
    "import gradio as gr\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "## Chain Dictionary\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "\n",
    "def output_puller(inputs):\n",
    "    \"\"\"If you want to support streaming, implement final step as a generator extractor.\"\"\"\n",
    "    for token in inputs:\n",
    "        if token.get('output'):\n",
    "            yield token.get('output')\n",
    "\n",
    "## Necessary Endpoints\n",
    "chains_dict = {\n",
    "    'basic' : RemoteRunnable(\"http://lab:9012/basic_chat/\"),\n",
    "    'retriever' : RemoteRunnable(\"http://lab:9012/retriever/\"),\n",
    "    'generator' : RemoteRunnable(\"http://lab:9012/generator/\"),\n",
    "}\n",
    "\n",
    "basic_chain = chains_dict['basic']\n",
    "\n",
    "\n",
    "## Retrieval-Augmented Generation Chain\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    | RunnableAssign(\n",
    "        {'context' : itemgetter('input') \n",
    "        | chains_dict['retriever'] \n",
    "        | LongContextReorder().transform_documents\n",
    "        | docs2str\n",
    "    })\n",
    ")\n",
    "\n",
    "output_chain = RunnableAssign({\"output\" : chains_dict['generator'] }) | output_puller\n",
    "rag_chain = retrieval_chain | output_chain\n",
    "\n",
    "#####################################################################\n",
    "## ChatBot utilities\n",
    "\n",
    "def add_message(message, history, role=0, preface=\"\"):\n",
    "    if not history or history[-1][role] is not None:\n",
    "        history += [[None, None]]\n",
    "    history[-1][role] = preface\n",
    "    buffer = \"\"\n",
    "    try:\n",
    "        for chunk in message:\n",
    "            token = getattr(chunk, 'content', chunk)\n",
    "            buffer += token\n",
    "            history[-1][role] += token\n",
    "            yield history, buffer, False \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gradio Stream failed: {e}\\nFor Input {history}\")\n",
    "        history[-1][role] += f\"...\\nGradio Stream failed: {e}\"\n",
    "        yield history, buffer, True\n",
    "\n",
    "\n",
    "def add_text(history, text):\n",
    "    history = history + [(text, None)]\n",
    "    return history, gr.Textbox(value=\"\", interactive=False)\n",
    "\n",
    "\n",
    "def bot(history, chain_key):\n",
    "    chain = {'Basic' : basic_chain, 'RAG' : rag_chain}.get(chain_key)\n",
    "    msg_stream = chain.stream(history[-1][0])\n",
    "    for history, buffer, is_error in add_message(msg_stream, history, role=1):\n",
    "        yield history\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "## Document/Assessment Utilities\n",
    "\n",
    "\n",
    "def get_chunks(document):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    "    )\n",
    "    content = document[0].page_content\n",
    "    content = content.replace(\"{\", \"[\").replace(\"}\", \"]\")\n",
    "    if \"References\" in content:\n",
    "        content = content[:content.index(\"References\")]\n",
    "    document[0].page_content = content\n",
    "    return text_splitter.split_documents(document)\n",
    "\n",
    "\n",
    "def get_day_difference(date_str):\n",
    "    given_date = datetime.strptime(date_str, '%Y-%m-%d').date()\n",
    "    current_date = datetime.now().date()\n",
    "    difference = current_date - given_date\n",
    "    return difference.days\n",
    "\n",
    "\n",
    "def get_fresh_chunks(chunks):\n",
    "    return [\n",
    "        chunk for chunk in chunks \n",
    "            if get_day_difference(chunk.metadata.get(\"Published\", \"2000-01-01\")) < 30\n",
    "    ]\n",
    "\n",
    "\n",
    "def format_chunk(doc):\n",
    "    prep_str = lambda x: x.replace('{', '<').replace('}', '>')\n",
    "    return (\n",
    "        f\"Paper: {prep_str(doc.metadata.get('Title', 'unknown'))}\"\n",
    "        f\"\\n\\nSummary: {prep_str(doc.metadata.get('Summary', 'unknown'))}\"\n",
    "        f\"\\n\\nPage Body: {prep_str(doc.page_content)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_synth_prompt(docs):\n",
    "    doc1, doc2 = random.sample(docs, 2)\n",
    "    sys_msg = (\n",
    "        \"Use the documents provided by the user to generate an interesting question-answer pair.\"\n",
    "        \" Try to use both documents if possible, and rely more on the document bodies than the summary. Be specific!\"\n",
    "        \" Use the format:\\nQuestion: (good question, 1-3 sentences, detailed)\\n\\nAnswer: (answer derived from the documents)\"\n",
    "    )\n",
    "    usr_msg = (f\"Document1: {format_chunk(doc1)}\\n\\nDocument2: {format_chunk(doc2)}\")\n",
    "    return ChatPromptTemplate.from_messages([('system', sys_msg), ('user', usr_msg)])\n",
    "\n",
    "\n",
    "def get_eval_prompt():\n",
    "    eval_instruction = (\n",
    "        \"Evaluate the following Question-Answer pair for human preference and consistency.\"\n",
    "        \"\\nAssume the first answer is a ground truth answer and has to be correct.\"\n",
    "        \"\\nAssume the second answer may or may not be true.\"\n",
    "        \"\\n[1] The first answer is extremely preferable, or the second answer heavily deviates.\"\n",
    "        \"\\n[2] The second answer does not contradict the first and significantly improves upon it.\"\n",
    "        \"\\n\\nOutput Format:\"\n",
    "        \"\\nJustification\\n[2] if 2 is strongly preferred, [1] otherwise\"\n",
    "    )\n",
    "    return {\"input\" : lambda x:x} | ChatPromptTemplate.from_messages([('system', eval_instruction), ('user', '{input}')])\n",
    "\n",
    "\n",
    "## Document names, and the overall chunk list\n",
    "class Globals:\n",
    "    doc_names = set()\n",
    "    doc_chunks = []\n",
    "\n",
    "\n",
    "def rag_eval(history, chain_key):\n",
    "    \"\"\"RAG Evaluation Chain\"\"\"\n",
    "    if not len(history) or history[-1][0] is not None:\n",
    "        history += [[None, None]]\n",
    "    \n",
    "    if not Globals.doc_chunks:\n",
    "        try: \n",
    "            docstore = FAISS.load_local(\"/notebooks/docstore_index\", lambda x:x)\n",
    "            Globals.doc_chunks = list(docstore.docstore._dict.values())\n",
    "            Globals.doc_names = {doc.metadata.get(\"Title\", \"Unknown\") for doc in Globals.doc_chunks}\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "    doc_names = Globals.doc_names \n",
    "    doc_chunks = get_fresh_chunks(Globals.doc_chunks)\n",
    "\n",
    "    if len(doc_chunks) < 2:\n",
    "        logger.error(f\"Attempted to evaluate with less than two fresh chunks submitted (last modified < 30 days ago)\")\n",
    "        history[-1][1] = \"Please upload a fresh paper (<30 days) inside your saved docstore_index directory that so we can ask our chain some questions\"\n",
    "        yield history\n",
    "    else:\n",
    "        main_chain = {'Basic' : basic_chain, 'RAG' : rag_chain}.get(chain_key)\n",
    "        eval_llm = basic_chain\n",
    "        num_points = 0\n",
    "        num_questions = 8\n",
    "\n",
    "        for i in range(num_questions):\n",
    "\n",
    "            synth_chain = get_synth_prompt(doc_chunks) | eval_llm\n",
    "            \n",
    "            preface = \"Generating Synthetic QA Pair:\\n\"\n",
    "            msg_stream = synth_chain.stream({})\n",
    "            for history, synth_qa, is_error in add_message(msg_stream, history, role=0, preface=preface):\n",
    "                yield history\n",
    "            if is_error: break\n",
    "\n",
    "            synth_pair = synth_qa.split(\"\\n\\n\")\n",
    "            if len(synth_pair) < 2:\n",
    "                logger.error(f\"Illegal QA with no break\")\n",
    "                history[-1][0] += f\"...\\nIllegal QA with no break\"\n",
    "                yield history\n",
    "            else:   \n",
    "                synth_q, synth_a = synth_pair[:2]\n",
    "\n",
    "                msg_stream = main_chain.stream(synth_q)\n",
    "                for history, rag_response, is_error in add_message(msg_stream, history, role=1):\n",
    "                    yield history\n",
    "                if is_error: break\n",
    "\n",
    "                eval_chain = get_eval_prompt() | eval_llm\n",
    "                usr_msg = f\"Question: {synth_q}\\n\\nAnswer 1: {synth_a}\\n\\n Answer 2: {rag_response}\"\n",
    "                msg_stream = eval_chain.stream(usr_msg)\n",
    "                for history, eval_response, is_error in add_message(msg_stream, history, role=0, preface=\"Evaluation: \"):\n",
    "                    yield history\n",
    "\n",
    "                num_points += (\"[2]\" in eval_response)\n",
    "            \n",
    "            history[-1][0] += f\"\\n[{num_points} / {i+1}]\"\n",
    "        \n",
    "        if (num_points / num_questions > 0.60):\n",
    "            msg_stream = (\n",
    "                \"Congrats! You've passed the assessment!! 😁\\n\"\n",
    "                \"Please make sure to click the ASSESS TASK button before shutting down your course environment\"\n",
    "            )\n",
    "            for history, eval_response, is_error in add_message(msg_stream, history, role=0):\n",
    "                yield history\n",
    "\n",
    "            ## secret\n",
    "\n",
    "        else: \n",
    "            msg_stream = f\"Metric score of {num_points / num_questions}, while 0.60 is required\\n\"\n",
    "            for history, eval_response, is_error in add_message(msg_stream, history, role=0):\n",
    "                yield history            \n",
    "        \n",
    "        yield history\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "## GRADIO EVENT LOOP\n",
    "\n",
    "# https://github.com/gradio-app/gradio/issues/4001\n",
    "CSS =\"\"\"\n",
    ".contain { display: flex; flex-direction: column; height:80vh;}\n",
    "#component-0 { height: 100%; }\n",
    "#chatbot { flex-grow: 1; overflow: auto;}\n",
    "\"\"\"\n",
    "THEME = gr.themes.Default(primary_hue=\"green\")\n",
    "\n",
    "with gr.Blocks(css=CSS, theme=THEME) as demo:\n",
    "    chatbot = gr.Chatbot(\n",
    "        [],\n",
    "        elem_id=\"chatbot\",\n",
    "        bubble_full_width=False,\n",
    "        avatar_images=(None, (os.path.join(os.path.dirname(__file__), \"parrot.png\"))),\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(\n",
    "            scale=4,\n",
    "            show_label=False,\n",
    "            placeholder=\"Enter text and press enter, or upload an image\",\n",
    "            container=False,\n",
    "        )\n",
    "\n",
    "        chain_btn  = gr.Radio([\"Basic\", \"RAG\"], value=\"Basic\", label=\"Main Route\")\n",
    "        test_btn   = gr.Button(\"🎓\\nEvaluate\")\n",
    "\n",
    "    # Reference: https://www.gradio.app/guides/blocks-and-event-listeners\n",
    "\n",
    "    # This listener is triggered when the user presses the Enter key while the Textbox is focused.\n",
    "    txt_msg = (\n",
    "        # first update the chatbot with the user message immediately. Also, disable the textbox\n",
    "        txt.submit(              ## On textbox submit (or enter)...\n",
    "            fn=add_text,            ## Run the add_text function...\n",
    "            inputs=[chatbot, txt],  ## Pass in the values of chatbot and txt...\n",
    "            outputs=[chatbot, txt], ## Assign the results to the values of chatbot and txt...\n",
    "            queue=False             ## And don't use the function as a generator (so no streaming)!\n",
    "        )\n",
    "        # then update the chatbot with the bot response (same variable logic)\n",
    "        .then(bot, [chatbot, chain_btn], [chatbot])\n",
    "        ## Then, unblock the textbox by assigning an active status to it\n",
    "        .then(lambda: gr.Textbox(interactive=True), None, [txt], queue=False)\n",
    "    )\n",
    "\n",
    "    test_msg = test_btn.click(\n",
    "        rag_eval, \n",
    "        inputs=[chatbot, chain_btn], \n",
    "        outputs=chatbot, \n",
    "    )\n",
    "\n",
    "#####################################################################\n",
    "## Final App Deployment\n",
    "\n",
    "demo.queue()\n",
    "\n",
    "logger.warning(\"Starting FastAPI app\")\n",
    "app = FastAPI()\n",
    "\n",
    "llm = ChatNVIDIA(model=\"mixtral_8x7b\")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    llm,\n",
    "    path=\"/basic_chat\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    llm,\n",
    "    path=\"/retriever\",\n",
    ")\n",
    "\n",
    "add_routes(\n",
    "    app,\n",
    "    llm,\n",
    "    path=\"/generator\",\n",
    ")\n",
    "\n",
    "app = gr.mount_gradio_app(app, demo, '/')\n",
    "\n",
    "@app.route(\"/health\")\n",
    "async def health():\n",
    "    return {\"success\": True}, 200\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=9012)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "u2xDAYn1qi_D",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2xDAYn1qi_D",
    "outputId": "ef35c8f4-210c-4c10-82e5-a3de2bfe1835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain/_api/module_import.py:120: LangChainDeprecationWarning: Importing ArxivLoader from langchain.document_loaders is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.document_loaders import ArxivLoader\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.document_loaders import ArxivLoader\n",
      "\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.11/site-packages/langchain/_api/module_import.py:120: LangChainDeprecationWarning: Importing LongContextReorder from langchain.document_transformers is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.document_transformers import LongContextReorder\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.document_transformers import LongContextReorder\n",
      "\n",
      "  warn_deprecated(\n",
      "Starting FastAPI app\n",
      "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m623\u001b[0m]\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\n",
      " __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n",
      "|  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n",
      "|  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n",
      "|  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n",
      "|  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n",
      "|_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n",
      "\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/basic_chat/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /basic_chat/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/generator/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /generator/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m Playground for chain \"/retriever/\" is live at:\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  │\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m  └──> /retriever/playground/\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m\n",
      "\u001b[1;32;40mLANGSERVE:\u001b[0m See all available routes at /docs/\n",
      "\n",
      "\u001b[1;31;40mLANGSERVE:\u001b[0m ⚠️ Using pydantic 2.7.1. OpenAPI docs for invoke, batch, stream, stream_log endpoints will not be generated. API endpoints and playground should work as expected. If you need to see the docs, you can downgrade to pydantic 1. For example, `pip install pydantic==1.10.13`. See https://github.com/tiangolo/fastapi/issues/10360 for details.\n",
      "\n",
      "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[31mERROR\u001b[0m:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 9012): address already in use\n",
      "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
      "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "## Works, but will block the notebook.\n",
    "!python server_app.py  \n",
    "\n",
    "## Will technically work, but not recommended in a notebook. \n",
    "## You may be surprised at the interesting side effects...\n",
    "# import os\n",
    "# os.system(\"python server_app.py &\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g9uRMEOrsy1d",
   "metadata": {
    "id": "g9uRMEOrsy1d"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Part 2:** Using The Server:\n",
    "\n",
    "While this cannot be easily utilized within Google Colab (or at least not without a lot of special tricks), the above script will keep a running server tied to the notebook process. While the server is running, do not attempt to use this notebook (except to shut down/restart the service).\n",
    "\n",
    "In another file, however, you should be able to access the `basic_chat` endpoint using the following interface:\n",
    "\n",
    "```python\n",
    "from langserve import RemoteRunnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = RemoteRunnable(\"http://0.0.0.0:9012/basic_chat/\") | StrOutputParser()\n",
    "for token in llm.stream(\"Hello World! How is it going?\"):\n",
    "    print(token, end='')\n",
    "```\n",
    "\n",
    "**Please try it out in a different file and see if it works!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c72a8e-3b5b-4442-a6aa-b94b839cacb2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c642796-2fc1-4ddf-a96a-35c0dbb3a54a",
   "metadata": {
    "id": "2c642796-2fc1-4ddf-a96a-35c0dbb3a54a"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
